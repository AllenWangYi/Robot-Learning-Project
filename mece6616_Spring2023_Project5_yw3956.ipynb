{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AllenWangYi/test/blob/main/mece6616_Spring2023_Project5_yw3956.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp8PVN5sJ9pd"
      },
      "source": [
        "# ***Important***\n",
        "\n",
        "**Before starting, make sure to read the [Assignment Instructions](https://courseworks2.columbia.edu/courses/172081/pages/assignment-instructions) page on Courseworks2 to learn the workflow for completing this project.**\n",
        "\n",
        "- *Apart from the link to your notebook, you are also required to submit `q_network.pth` of Part 1 and `ppo_network.zip` (model checkpoints are loaded and saved by stable_baselines3 as zip files) of Part 2 to Coursework. You should put the link to your notebook in the comment entry.*\n",
        "- *Please name the revision you want us to grade \"Grade me\". We will only grade the revision that is correctly named. Late days will be applied according to this named revision.*\n",
        "- *Please make sure that \"anyone in LionMail with the link\" has Edit permissions.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inY7y5CRo97q"
      },
      "source": [
        "# Project Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPIiNSZ8hb8Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "face8b38-4003-4e51-aa16-bf63b6a4bd97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mecs6616_sp23_project5'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 43 (delta 19), reused 23 (delta 7), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (43/43), 13.83 KiB | 1011.00 KiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# After running this cell, the folder 'mecs6616_sp23_project3' will show up in the file explorer on the left (click on the folder icon if it's not open)\n",
        "# It may take a few seconds to appear\n",
        "!git clone https://github.com/roamlab/mecs6616_sp23_project5.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ise8RAQhhs3X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "043ebd7d-79ec-46f1-ad11-61bb49acc61e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/mecs6616_sp23_project5/arm_dynamics_base.py' -> '/content/arm_dynamics_base.py'\n",
            "'/content/mecs6616_sp23_project5/arm_dynamics.py' -> '/content/arm_dynamics.py'\n",
            "'/content/mecs6616_sp23_project5/arm_env.py' -> '/content/arm_env.py'\n",
            "'/content/mecs6616_sp23_project5/geometry.py' -> '/content/geometry.py'\n",
            "'/content/mecs6616_sp23_project5/render.py' -> '/content/render.py'\n",
            "'/content/mecs6616_sp23_project5/robot.py' -> '/content/robot.py'\n",
            "'/content/mecs6616_sp23_project5/score.py' -> '/content/score.py'\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# copy all needed files into the working directory. This is simply to make accessing files easier\n",
        "!cp -av /content/mecs6616_sp23_project5/* /content/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# There will be error messages from this command. You can ignore those error messages\n",
        "# as long as you see \"Successfully installed setuptools-65.5.0\" at the end.\n",
        "\n",
        "!pip install setuptools==65.5.0"
      ],
      "metadata": {
        "id": "gYlcOXqbosEA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57f0e655-015a-4b48-f686-dcf74ee4f255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting setuptools==65.5.0\n",
            "  Downloading setuptools-65.5.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "cvxpy 1.3.1 requires setuptools>65.5.1, but you have setuptools 65.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed setuptools-65.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# There will be error messages from this command. You can ignore those error messages\n",
        "# as long as you see \"Successfully installed gym-0.21.0 stable-baselines3-1.5.0\" at the end.\n",
        "\n",
        "!pip install gym==0.21.0 stable-baselines3==1.5.0"
      ],
      "metadata": {
        "id": "RChyEFYYqLGl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c7ced0-9542-44a5-d47a-133e5141969f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym==0.21.0\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting stable-baselines3==1.5.0\n",
            "  Downloading stable_baselines3-1.5.0-py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.7/177.7 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.21.0) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.21.0) (2.2.1)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==1.5.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==1.5.0) (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==1.5.0) (1.5.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->stable-baselines3==1.5.0) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->stable-baselines3==1.5.0) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->stable-baselines3==1.5.0) (3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->stable-baselines3==1.5.0) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->stable-baselines3==1.5.0) (4.5.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->stable-baselines3==1.5.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->stable-baselines3==1.5.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->stable-baselines3==1.5.0) (16.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==1.5.0) (23.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==1.5.0) (1.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==1.5.0) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==1.5.0) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==1.5.0) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==1.5.0) (4.39.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==1.5.0) (2.8.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==1.5.0) (8.4.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3==1.5.0) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==1.5.0) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->stable-baselines3==1.5.0) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->stable-baselines3==1.5.0) (1.3.0)\n",
            "Building wheels for collected packages: gym\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for gym\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for gym\n",
            "Failed to build gym\n",
            "Installing collected packages: gym, stable-baselines3\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "  Running setup.py install for gym ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: gym was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. pip 23.1 will enforce this behaviour change. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed gym-0.21.0 stable-baselines3-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Implement DQN\n",
        "\n",
        "For this part, you will implement DQN from scratch. You SHOULD NOT use any RL libraries."
      ],
      "metadata": {
        "id": "-KNg9fzU5Un7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-JvzRuwNsYz"
      },
      "source": [
        "## Starter Code Explanation\n",
        "In addition to code you are already familiar with from the previous project (i.e. arm dynamics, etc.) we are providing an \"Environment\" in the `ArmEnv` class. The environment \"wraps around\" the arm dynamics and provides the key functions that an RL algorithm expects: reset(...) and step(...). The implementation of `ArmEnv` follows the [OpenAI Gym](https://www.gymlibrary.dev/api/core/) API standard. It is a standard that is accepeted by many RL libraries and allows for our problem to be easily solved with various RL libraries. Take a moment to familiarize yourself with these functions! See [here](https://www.gymlibrary.dev/api/core/) for more information on the definition of the reset(...) and step(...) functions.\n",
        "\n",
        "Important notes:\n",
        "\n",
        "* The ArmEnv expects an action similar to the one used previously: a vector with a torque for every arm joint. Thus, the native action space for this environment is high-dimensional, and continuous. DQN will require an action space that is 1-dimensional and discrete. You will need to convert between these. For example, you can have an action space of [0, 1, 2,] where each number just represents the identity of an action candidate, and a conversion dictionary {0: [-0.1, -0.1], 1: [0.1, 0.1], 2: [0, 0]}. Then, when the Q network output an action 1, it will be converted into [0.1, 0.1] and used by the environment. Note that this is just an example method to implement the conversion and you do not have to follow the same procedure.\n",
        "* The observation provided by the environment will comprise the same state vector as before, to which we append the current position of the end_effector and the goal for the end-effector. Since your policy must learn to reach arbitrary goals, the goal must be provided as part of the observation. So the observation will consist of 8 values: 4 for the state, 2 for the pos_ee and 2 for the goal.\n",
        "* The maximum episode length of the environment is 200 steps. Each step is simulated for 0.01 second. This should be used for both training and testing.\n",
        "* The reward function of this environment is by default r(s, a) = - dist(pos_ee, goal)^2 where represents the negative square of L2 distance between the current position of the end-effector and the goal position."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Arm Environment Example\n",
        "You are encouraged to view the `arm_env.py` file to understand the `random_goal()`, `reset()` and `step()`  functions but do not modify the file.\n",
        "\n",
        "The `env.reset()` method, will reset the arm in the vertically downwards position and set a new random goal by calling the `random_goal()` method. By understanding how the goals are set you could guide your training in that direction. You can also provide your own goal as a (2,1) array to the reset function as an argument. This could come handy later when training the model.\n",
        "\n",
        "The `env.step()` function takes an action as a (2,1) shaped array and outputs the next observation, reward, done and info. `info` is a dictionary with pos_ee and vel_ee values. This can come handy if you attempt to do some reward engineering.\n",
        "\n",
        "The cell below provides an example of random policy interacting with the ArmEnv for 50 steps (0.5 seconds)"
      ],
      "metadata": {
        "id": "gw8H0PZcSv7F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830
        },
        "outputId": "b1c9b41d-6d5a-4106-85fd-554cd2eb3b03",
        "id": "o6r9kJ5jpeds"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from render import Renderer\n",
        "from arm_env import ArmEnv\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "import numpy as np\n",
        "\n",
        "# DO NOT CHANGE arm parameters\n",
        "arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01,\n",
        "\t    \t\t\tgravity=False\n",
        "        )\n",
        "    )\n",
        "arm.reset()\n",
        "# ------------------\n",
        "\n",
        "env = ArmEnv(arm, gui=True)\n",
        "\n",
        "# Passing our own defined goal to the reset function\n",
        "# goal = np.array([[0.5], [-1.5]])\n",
        "# obs = env.reset(goal)\n",
        "\n",
        "# Resetting the environment without the goal will set a random goal position\n",
        "obs = env.reset()\n",
        "\n",
        "for _ in range(50):\n",
        "  rand_action = np.random.uniform(-1.5, 1.5, (2,1))\n",
        "  obs, reward, done, info = env.step(rand_action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jmXTT_ngdqG"
      },
      "source": [
        "### QNetwork\n",
        "This class defines the architecture of your network. You must fill in the __init__(...) function which defines your network, and the forward(...) function which performs the forward pass.\n",
        "\n",
        "Your action space should be discrete, with whatever cardinality you decide. The size of the output layer of your Q-Network should thus be the same as the cardinality of your action space. When selecting an action, a policy must choose the one that has the highest estimated Q-value for the current state. As part of the QNetwork class, we are providing the function select_discrete_action(...) which does exactly that.\n",
        "\n",
        "The arm environment itself however expects a 2-dimensional, continuous action vector. Therefore, when it comes to send an action to the environment, you must provide the kind of action the environment expects. It is your job to determine how to convert between the discrete action space of your Q-Network and the continuous action space of the arm. You do this by filling in the action_discrete_to_continuous(...) function in your QNetwork. You can expect to call the step function of the environment like this:\n",
        "\n",
        "```\n",
        "self.env.step(self.q_network.action_discrete_to_continuous(discrete_action))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UyguLRKgf_I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "  def __init__(self, env):\n",
        "    super(QNetwork, self).__init__()\n",
        "    #--------- YOUR CODE HERE --------------\n",
        "    self.fc1 = nn.Linear(env.observation_space.shape[0], 64)\n",
        "    self.fc2 = nn.Linear(64, 128)\n",
        "    self.fc3 = nn.Linear(128, 289)\n",
        "    #---------------------------------------\n",
        "\n",
        "  def forward(self, x, device):\n",
        "    #--------- YOUR CODE HERE --------------\n",
        "    x = torch.FloatTensor(x)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "    #---------------------------------------\n",
        "\n",
        "\n",
        "  def select_discrete_action(self, obs, device):\n",
        "    # Put the observation through the network to estimate q values for all possible discrete actions\n",
        "    est_q_vals = self.forward(obs.reshape((1,) + obs.shape), device)\n",
        "    # Choose the discrete action with the highest estimated q value\n",
        "    discrete_action = torch.argmax(est_q_vals, dim=1).tolist()[0]\n",
        "    return discrete_action\n",
        "\n",
        "  def action_discrete_to_continuous(self, discrete_action):\n",
        "    #--------- YOUR CODE HERE --------------\n",
        "\n",
        "    x = np.linspace(-0.8, 0.8, 17, dtype=np.float64)\n",
        "    y = np.linspace(-0.8, 0.8, 17, dtype=np.float64)\n",
        "    arr = np.vstack(np.meshgrid(x, y)).reshape(2, -1).T\n",
        "    arr = np.around(arr, decimals=10)\n",
        "\n",
        "    candidate = {i: arr[i].tolist() for i in range(len(arr))}\n",
        "\n",
        "\n",
        "    action = candidate[discrete_action]\n",
        "\n",
        "    return np.array(action).reshape(2, -1)\n",
        "    #---------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We provide you with code to use the replay buffer in your RL implementation. You do not need to change the ReplayBuffer class.\n",
        "```\n",
        "rb = ReplayBuffer()\n",
        "```\n",
        "After creating a ReplayBuffer object you can add samples in the buffer using `put()`:\n",
        "```\n",
        "rb.put((obs, action, reward, next_obs, done))\n",
        "```\n",
        "Take random samples from the buffer using:\n",
        "```\n",
        "obs, actions, rewards, next_obses, dones = rb.sample(batch_size)\n",
        "```\n"
      ],
      "metadata": {
        "id": "IUjAeQcPdsGR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7NytRAXtYkE"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ReplayBuffer():\n",
        "    def __init__(self, buffer_limit):\n",
        "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
        "\n",
        "    def put(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, n):\n",
        "        mini_batch = random.sample(self.buffer, n)\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
        "\n",
        "        for transition in mini_batch:\n",
        "            s, a, r, s_prime, done_mask = transition\n",
        "            s_lst.append(s)\n",
        "            a_lst.append(a)\n",
        "            r_lst.append(r)\n",
        "            s_prime_lst.append(s_prime)\n",
        "            done_mask_lst.append(done_mask)\n",
        "\n",
        "        return np.array(s_lst), np.array(a_lst), \\\n",
        "               np.array(r_lst), np.array(s_prime_lst), \\\n",
        "               np.array(done_mask_lst)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TrainDQN\n",
        "Here, you must fill in the train(...) function that actually trains your network.\n",
        "\n",
        "We are providing a helper function called save_model(...) that will save the current Q-network. Use this as you see fit.\n",
        "\n",
        "To set one network equal to another one, you can use code like this:\n",
        "```\n",
        "target_network.load_state_dict(self.q_network.state_dict())\n",
        "```\n",
        "\n",
        "If you would like to be graded with a specific seed for the random number generators, make sure to change the default seed in the initialization of the TrainDQN class.\n",
        "\n",
        "The time taken to train the model will depend mainly on how big is your model architecture and the number of episodes you run the training for. As a reference, the time taken to train a model on 1500 episodes, which passed all evaluation metrics was about an hour.\n",
        "* Reference value for clipping the gradient value as mentioned in class: 0.2\n",
        "* Reference value for a typical size of Replay Buffer: >10k\n",
        "* Reference value for batch size while training: 64 - 512\n",
        "\n",
        "Note that these are just reference values and larger is not always better as it may slow things down.\n",
        "\n",
        "It is good practice in RL to ensure simpler things are working before complicating environments or training techniques.\n",
        "\n",
        "If you think your training method is not working at all, you could pass a fixed goal to the `env.reset()` method during the training loop to ensure that your model is learning."
      ],
      "metadata": {
        "id": "pxVawoBLe3bd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwS8xVR7tbeQ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from render import Renderer\n",
        "from arm_env import ArmEnv\n",
        "import numpy as np\n",
        "from math import dist\n",
        "import os\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "class TrainDQN:\n",
        "\n",
        "  def __init__(self, env, seed=0):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    self.env = env\n",
        "    self.device = torch.device('cpu')\n",
        "    self.q_network = QNetwork(env).to(self.device)\n",
        "    self.target_network = QNetwork(env).to(self.device)\n",
        "    self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "    self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001)\n",
        "    self.buffer_limit = 10000\n",
        "    self.replay_buffer = ReplayBuffer(self.buffer_limit)\n",
        "    self.batch_size = 64\n",
        "    self.gamma = 0.9\n",
        "    self.target_update_interval = 25\n",
        "    self.epsilon = 1.0\n",
        "    self.epsilon_min = 0\n",
        "    self.epsilon_decay = 0.995\n",
        "\n",
        "  def save_model(self, episode_num, save_dir='models'):\n",
        "    timestr = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    model_dir = os.path.join(save_dir, timestr)\n",
        "    if not os.path.exists(os.path.join(model_dir)):\n",
        "      os.makedirs(os.path.join(model_dir))\n",
        "    savepath = os.path.join(model_dir, f'q_network_ep_{episode_num:04d}.pth')\n",
        "    torch.save(self.q_network.state_dict(), savepath)\n",
        "    print(f'model saved to {savepath}\\n')\n",
        "\n",
        "  def select_epsilon_greedy_action(self, obs):\n",
        "    if np.random.random() < self.epsilon:\n",
        "      # Random action\n",
        "      discrete_action = np.random.randint(0, self.q_network.fc3.out_features)\n",
        "    else:\n",
        "      # Action based on Q-network\n",
        "      discrete_action = self.q_network.select_discrete_action(obs, self.device)\n",
        "    return discrete_action\n",
        "\n",
        "  def train(self, num_episodes=1501):\n",
        "    for episode in range(num_episodes):\n",
        "      obs = self.env.reset()\n",
        "      done = False\n",
        "      total_reward = 0\n",
        "      steps = 0\n",
        "      while not done:\n",
        "        discrete_action = self.select_epsilon_greedy_action(obs)\n",
        "        continuous_action = self.q_network.action_discrete_to_continuous(discrete_action)\n",
        "        next_obs, reward, done, _ = self.env.step(continuous_action)\n",
        "        self.replay_buffer.put((obs, discrete_action, reward, next_obs, done))\n",
        "        obs = next_obs\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "\n",
        "        if len(self.replay_buffer.buffer) >= self.batch_size:\n",
        "          s, a, r, s_prime, done_mask = self.replay_buffer.sample(self.batch_size)\n",
        "          q_out = self.q_network(torch.tensor(s, dtype=torch.float), self.device)\n",
        "          q_a = q_out.gather(1, torch.tensor(a).unsqueeze(1))\n",
        "          max_q_prime = self.target_network(torch.tensor(s_prime, dtype=torch.float), self.device).max(1)[0].unsqueeze(1)\n",
        "          r_tensor = torch.tensor(r, dtype=torch.float).unsqueeze(1)\n",
        "          done_mask_tensor = torch.tensor(1 - done_mask, dtype=torch.float).unsqueeze(1)\n",
        "          target = r_tensor + self.gamma * max_q_prime * done_mask_tensor\n",
        "          # print('qa',q_a.shape)\n",
        "          # print('target',target.shape)\n",
        "          loss = F.mse_loss(q_a, target)\n",
        "          self.optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 0.2)\n",
        "          self.optimizer.step()\n",
        "\n",
        "      self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "      if episode % self.target_update_interval == 0:\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "      print(f\"Episode: {episode}, Reward: {total_reward}, Steps: {steps}\")\n",
        "      # if episode % 100 == 0:\n",
        "      if episode > 1400:\n",
        "        self.save_model(episode)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEHSV1Q1BT1t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7d74e6f-c3f7-4a03-8cc7-10ddac1c0b08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0, Reward: -549.0667904869887, Steps: 200\n",
            "Episode: 1, Reward: -790.5248886397507, Steps: 200\n",
            "Episode: 2, Reward: -677.2620227344321, Steps: 200\n",
            "Episode: 3, Reward: -2020.8713061513026, Steps: 200\n",
            "Episode: 4, Reward: -1574.7161406670295, Steps: 200\n",
            "Episode: 5, Reward: -950.3425755338022, Steps: 200\n",
            "Episode: 6, Reward: -486.43278500931706, Steps: 200\n",
            "Episode: 7, Reward: -1259.5732145823674, Steps: 200\n",
            "Episode: 8, Reward: -1140.8921953636825, Steps: 200\n",
            "Episode: 9, Reward: -423.3928283696597, Steps: 200\n",
            "Episode: 10, Reward: -1109.715257159733, Steps: 200\n",
            "Episode: 11, Reward: -816.706180395644, Steps: 200\n",
            "Episode: 12, Reward: -1245.0228619061907, Steps: 200\n",
            "Episode: 13, Reward: -692.9163004025287, Steps: 200\n",
            "Episode: 14, Reward: -656.943182882173, Steps: 200\n",
            "Episode: 15, Reward: -472.5173689311424, Steps: 200\n",
            "Episode: 16, Reward: -723.9221855428008, Steps: 200\n",
            "Episode: 17, Reward: -599.9833878783295, Steps: 200\n",
            "Episode: 18, Reward: -974.875506943275, Steps: 200\n",
            "Episode: 19, Reward: -1289.9943847844625, Steps: 200\n",
            "Episode: 20, Reward: -775.4555155841919, Steps: 200\n",
            "Episode: 21, Reward: -1045.1278102047308, Steps: 200\n",
            "Episode: 22, Reward: -1098.792247100405, Steps: 200\n",
            "Episode: 23, Reward: -998.0452835276242, Steps: 200\n",
            "Episode: 24, Reward: -993.4749613000736, Steps: 200\n",
            "Episode: 25, Reward: -133.20365920591578, Steps: 200\n",
            "Episode: 26, Reward: -1100.1707413620766, Steps: 200\n",
            "Episode: 27, Reward: -1305.0158996888083, Steps: 200\n",
            "Episode: 28, Reward: -781.2766712653035, Steps: 200\n",
            "Episode: 29, Reward: -736.0791142658821, Steps: 200\n",
            "Episode: 30, Reward: -1075.6391880750716, Steps: 200\n",
            "Episode: 31, Reward: -1424.0096626798634, Steps: 200\n",
            "Episode: 32, Reward: -1160.9392483943939, Steps: 200\n",
            "Episode: 33, Reward: -507.3958246629324, Steps: 200\n",
            "Episode: 34, Reward: -704.9039907980277, Steps: 200\n",
            "Episode: 35, Reward: -1108.3335040588022, Steps: 200\n",
            "Episode: 36, Reward: -1149.5324332184045, Steps: 200\n",
            "Episode: 37, Reward: -1312.0200510474492, Steps: 200\n",
            "Episode: 38, Reward: -1088.7185160669221, Steps: 200\n",
            "Episode: 39, Reward: -1742.3125167505364, Steps: 200\n",
            "Episode: 40, Reward: -1219.067225932974, Steps: 200\n",
            "Episode: 41, Reward: -1169.7468049810927, Steps: 200\n",
            "Episode: 42, Reward: -1017.9924735670631, Steps: 200\n",
            "Episode: 43, Reward: -1069.1133274446768, Steps: 200\n",
            "Episode: 44, Reward: -677.7424112733021, Steps: 200\n",
            "Episode: 45, Reward: -632.4968853404735, Steps: 200\n",
            "Episode: 46, Reward: -424.4954610389056, Steps: 200\n",
            "Episode: 47, Reward: -480.9304538265828, Steps: 200\n",
            "Episode: 48, Reward: -1061.3600505836387, Steps: 200\n",
            "Episode: 49, Reward: -879.6452987665225, Steps: 200\n",
            "Episode: 50, Reward: -841.2546107001289, Steps: 200\n",
            "Episode: 51, Reward: -1209.1316218482275, Steps: 200\n",
            "Episode: 52, Reward: -1287.307641650328, Steps: 200\n",
            "Episode: 53, Reward: -1872.2699614995631, Steps: 200\n",
            "Episode: 54, Reward: -1182.560313692394, Steps: 200\n",
            "Episode: 55, Reward: -553.6906417699937, Steps: 200\n",
            "Episode: 56, Reward: -890.1855276417125, Steps: 200\n",
            "Episode: 57, Reward: -1172.8038272771282, Steps: 200\n",
            "Episode: 58, Reward: -959.6863767836693, Steps: 200\n",
            "Episode: 59, Reward: -617.4243991324395, Steps: 200\n",
            "Episode: 60, Reward: -743.1920652626491, Steps: 200\n",
            "Episode: 61, Reward: -909.6942710747347, Steps: 200\n",
            "Episode: 62, Reward: -668.157568127764, Steps: 200\n",
            "Episode: 63, Reward: -409.10786342876025, Steps: 200\n",
            "Episode: 64, Reward: -577.3468276655135, Steps: 200\n",
            "Episode: 65, Reward: -604.5571879756495, Steps: 200\n",
            "Episode: 66, Reward: -1207.3041460305456, Steps: 200\n",
            "Episode: 67, Reward: -557.9569165698118, Steps: 200\n",
            "Episode: 68, Reward: -682.5871162589389, Steps: 200\n",
            "Episode: 69, Reward: -1020.907012965053, Steps: 200\n",
            "Episode: 70, Reward: -801.0408844179643, Steps: 200\n",
            "Episode: 71, Reward: -873.6184682335054, Steps: 200\n",
            "Episode: 72, Reward: -1099.3079570489476, Steps: 200\n",
            "Episode: 73, Reward: -1325.523592330139, Steps: 200\n",
            "Episode: 74, Reward: -965.0427967479657, Steps: 200\n",
            "Episode: 75, Reward: -756.0242004235639, Steps: 200\n",
            "Episode: 76, Reward: -1049.8102648019049, Steps: 200\n",
            "Episode: 77, Reward: -1409.6421725595198, Steps: 200\n",
            "Episode: 78, Reward: -531.2759504228858, Steps: 200\n",
            "Episode: 79, Reward: -1637.0177902588111, Steps: 200\n",
            "Episode: 80, Reward: -928.0840277566742, Steps: 200\n",
            "Episode: 81, Reward: -1175.529276607859, Steps: 200\n",
            "Episode: 82, Reward: -1021.6945117556712, Steps: 200\n",
            "Episode: 83, Reward: -558.7129781732604, Steps: 200\n",
            "Episode: 84, Reward: -753.8613503377018, Steps: 200\n",
            "Episode: 85, Reward: -862.3744527200786, Steps: 200\n",
            "Episode: 86, Reward: -925.6864350665014, Steps: 200\n",
            "Episode: 87, Reward: -622.2437556387566, Steps: 200\n",
            "Episode: 88, Reward: -1355.8305975552516, Steps: 200\n",
            "Episode: 89, Reward: -1177.4632128341125, Steps: 200\n",
            "Episode: 90, Reward: -836.2793111578047, Steps: 200\n",
            "Episode: 91, Reward: -1164.7907947974209, Steps: 200\n",
            "Episode: 92, Reward: -921.7588919836355, Steps: 200\n",
            "Episode: 93, Reward: -1023.0218828015243, Steps: 200\n",
            "Episode: 94, Reward: -682.2758081474635, Steps: 200\n",
            "Episode: 95, Reward: -1083.7592810624137, Steps: 200\n",
            "Episode: 96, Reward: -1626.4705993017628, Steps: 200\n",
            "Episode: 97, Reward: -596.8610950059182, Steps: 200\n",
            "Episode: 98, Reward: -314.6607758284869, Steps: 200\n",
            "Episode: 99, Reward: -961.2077445524817, Steps: 200\n",
            "Episode: 100, Reward: -892.675735582074, Steps: 200\n",
            "Episode: 101, Reward: -715.0753431363908, Steps: 200\n",
            "Episode: 102, Reward: -760.6093932618293, Steps: 200\n",
            "Episode: 103, Reward: -997.4196393328506, Steps: 200\n",
            "Episode: 104, Reward: -1015.3958181052199, Steps: 200\n",
            "Episode: 105, Reward: -1277.1565972397213, Steps: 200\n",
            "Episode: 106, Reward: -924.0093020572763, Steps: 200\n",
            "Episode: 107, Reward: -929.7031428656164, Steps: 200\n",
            "Episode: 108, Reward: -1238.8156117873066, Steps: 200\n",
            "Episode: 109, Reward: -804.6157006450698, Steps: 200\n",
            "Episode: 110, Reward: -785.8376194080136, Steps: 200\n",
            "Episode: 111, Reward: -573.3909286109895, Steps: 200\n",
            "Episode: 112, Reward: -1007.5408005408427, Steps: 200\n",
            "Episode: 113, Reward: -987.5321787950664, Steps: 200\n",
            "Episode: 114, Reward: -489.2157761620224, Steps: 200\n",
            "Episode: 115, Reward: -810.2562654700151, Steps: 200\n",
            "Episode: 116, Reward: -934.1544339612701, Steps: 200\n",
            "Episode: 117, Reward: -1190.3964687207751, Steps: 200\n",
            "Episode: 118, Reward: -859.9692322683314, Steps: 200\n",
            "Episode: 119, Reward: -951.3979758021129, Steps: 200\n",
            "Episode: 120, Reward: -665.2005297091789, Steps: 200\n",
            "Episode: 121, Reward: -561.2264452183787, Steps: 200\n",
            "Episode: 122, Reward: -797.2435865920742, Steps: 200\n",
            "Episode: 123, Reward: -1420.4281353419622, Steps: 200\n",
            "Episode: 124, Reward: -684.2782706604797, Steps: 200\n",
            "Episode: 125, Reward: -1063.1220515164175, Steps: 200\n",
            "Episode: 126, Reward: -1258.9484147295498, Steps: 200\n",
            "Episode: 127, Reward: -523.0485650538856, Steps: 200\n",
            "Episode: 128, Reward: -849.1982838776944, Steps: 200\n",
            "Episode: 129, Reward: -849.5402698954682, Steps: 200\n",
            "Episode: 130, Reward: -960.0639384830731, Steps: 200\n",
            "Episode: 131, Reward: -896.7562352743788, Steps: 200\n",
            "Episode: 132, Reward: -365.2436502924654, Steps: 200\n",
            "Episode: 133, Reward: -885.15533330903, Steps: 200\n",
            "Episode: 134, Reward: -1080.7405656436847, Steps: 200\n",
            "Episode: 135, Reward: -751.5159428836525, Steps: 200\n",
            "Episode: 136, Reward: -584.9264190200265, Steps: 200\n",
            "Episode: 137, Reward: -469.9043759513735, Steps: 200\n",
            "Episode: 138, Reward: -894.1180935158409, Steps: 200\n",
            "Episode: 139, Reward: -891.1429904805489, Steps: 200\n",
            "Episode: 140, Reward: -752.2095344279454, Steps: 200\n",
            "Episode: 141, Reward: -341.68820154112797, Steps: 200\n",
            "Episode: 142, Reward: -782.5950624852626, Steps: 200\n",
            "Episode: 143, Reward: -247.76379507981218, Steps: 200\n",
            "Episode: 144, Reward: -500.66090684781614, Steps: 200\n",
            "Episode: 145, Reward: -588.1480719069842, Steps: 200\n",
            "Episode: 146, Reward: -1118.577545802595, Steps: 200\n",
            "Episode: 147, Reward: -600.4046834179608, Steps: 200\n",
            "Episode: 148, Reward: -461.93106365667086, Steps: 200\n",
            "Episode: 149, Reward: -647.0083506100825, Steps: 200\n",
            "Episode: 150, Reward: -977.5751077281934, Steps: 200\n",
            "Episode: 151, Reward: -939.6242862955362, Steps: 200\n",
            "Episode: 152, Reward: -999.1948163323689, Steps: 200\n",
            "Episode: 153, Reward: -455.8799576811079, Steps: 200\n",
            "Episode: 154, Reward: -593.5829912988484, Steps: 200\n",
            "Episode: 155, Reward: -644.4214518053645, Steps: 200\n",
            "Episode: 156, Reward: -768.4662467601354, Steps: 200\n",
            "Episode: 157, Reward: -155.2830562233011, Steps: 200\n",
            "Episode: 158, Reward: -666.7139119629477, Steps: 200\n",
            "Episode: 159, Reward: -619.4306756589062, Steps: 200\n",
            "Episode: 160, Reward: -562.2919647726197, Steps: 200\n",
            "Episode: 161, Reward: -876.3801973430192, Steps: 200\n",
            "Episode: 162, Reward: -586.0534164605492, Steps: 200\n",
            "Episode: 163, Reward: -995.766682867447, Steps: 200\n",
            "Episode: 164, Reward: -872.6015096397756, Steps: 200\n",
            "Episode: 165, Reward: -499.84831159620927, Steps: 200\n",
            "Episode: 166, Reward: -426.0552347181654, Steps: 200\n",
            "Episode: 167, Reward: -267.93143037135377, Steps: 200\n",
            "Episode: 168, Reward: -334.33882749357343, Steps: 200\n",
            "Episode: 169, Reward: -535.8052574687665, Steps: 200\n",
            "Episode: 170, Reward: -203.91968822420404, Steps: 200\n",
            "Episode: 171, Reward: -1290.9375426345096, Steps: 200\n",
            "Episode: 172, Reward: -329.0223374330039, Steps: 200\n",
            "Episode: 173, Reward: -773.1021512858804, Steps: 200\n",
            "Episode: 174, Reward: -514.2270274837441, Steps: 200\n",
            "Episode: 175, Reward: -425.71733986078385, Steps: 200\n",
            "Episode: 176, Reward: -623.9761407850143, Steps: 200\n",
            "Episode: 177, Reward: -1268.8137797984352, Steps: 200\n",
            "Episode: 178, Reward: -597.2110502056491, Steps: 200\n",
            "Episode: 179, Reward: -642.7395720244003, Steps: 200\n",
            "Episode: 180, Reward: -514.0640941726953, Steps: 200\n",
            "Episode: 181, Reward: -463.6054980792596, Steps: 200\n",
            "Episode: 182, Reward: -392.6727556248816, Steps: 200\n",
            "Episode: 183, Reward: -677.7788011797171, Steps: 200\n",
            "Episode: 184, Reward: -1441.205926987774, Steps: 200\n",
            "Episode: 185, Reward: -323.77969987236986, Steps: 200\n",
            "Episode: 186, Reward: -299.95831415100236, Steps: 200\n",
            "Episode: 187, Reward: -507.5236925189167, Steps: 200\n",
            "Episode: 188, Reward: -536.950233665607, Steps: 200\n",
            "Episode: 189, Reward: -591.8914008827949, Steps: 200\n",
            "Episode: 190, Reward: -282.5804707249485, Steps: 200\n",
            "Episode: 191, Reward: -643.0310452659022, Steps: 200\n",
            "Episode: 192, Reward: -898.5834904845212, Steps: 200\n",
            "Episode: 193, Reward: -892.9949331711114, Steps: 200\n",
            "Episode: 194, Reward: -976.9777011613379, Steps: 200\n",
            "Episode: 195, Reward: -330.61765034801266, Steps: 200\n",
            "Episode: 196, Reward: -125.98778118526235, Steps: 200\n",
            "Episode: 197, Reward: -406.4744328163586, Steps: 200\n",
            "Episode: 198, Reward: -294.7975442346891, Steps: 200\n",
            "Episode: 199, Reward: -371.5705744603005, Steps: 200\n",
            "Episode: 200, Reward: -401.03452370545403, Steps: 200\n",
            "Episode: 201, Reward: -402.0866883454652, Steps: 200\n",
            "Episode: 202, Reward: -867.2557504014244, Steps: 200\n",
            "Episode: 203, Reward: -609.8814114541334, Steps: 200\n",
            "Episode: 204, Reward: -129.97757819391524, Steps: 200\n",
            "Episode: 205, Reward: -109.77448891346418, Steps: 200\n",
            "Episode: 206, Reward: -506.76319195053713, Steps: 200\n",
            "Episode: 207, Reward: -621.7412863782638, Steps: 200\n",
            "Episode: 208, Reward: -632.9270896405369, Steps: 200\n",
            "Episode: 209, Reward: -528.667744307932, Steps: 200\n",
            "Episode: 210, Reward: -358.23669361645204, Steps: 200\n",
            "Episode: 211, Reward: -377.55913275030207, Steps: 200\n",
            "Episode: 212, Reward: -439.27528876730145, Steps: 200\n",
            "Episode: 213, Reward: -304.7646776054287, Steps: 200\n",
            "Episode: 214, Reward: -612.8293321315887, Steps: 200\n",
            "Episode: 215, Reward: -400.5999595635192, Steps: 200\n",
            "Episode: 216, Reward: -452.77530098930873, Steps: 200\n",
            "Episode: 217, Reward: -330.5776897939437, Steps: 200\n",
            "Episode: 218, Reward: -289.98431465541876, Steps: 200\n",
            "Episode: 219, Reward: -737.4095290422947, Steps: 200\n",
            "Episode: 220, Reward: -501.96560751921857, Steps: 200\n",
            "Episode: 221, Reward: -333.80227263384865, Steps: 200\n",
            "Episode: 222, Reward: -396.7727961016061, Steps: 200\n",
            "Episode: 223, Reward: -404.68561992226466, Steps: 200\n",
            "Episode: 224, Reward: -456.14834623109806, Steps: 200\n",
            "Episode: 225, Reward: -383.76203125649647, Steps: 200\n",
            "Episode: 226, Reward: -290.84203549230705, Steps: 200\n",
            "Episode: 227, Reward: -305.3897973275398, Steps: 200\n",
            "Episode: 228, Reward: -479.271842117385, Steps: 200\n",
            "Episode: 229, Reward: -536.0699640605027, Steps: 200\n",
            "Episode: 230, Reward: -758.3345412183947, Steps: 200\n",
            "Episode: 231, Reward: -280.7374510063786, Steps: 200\n",
            "Episode: 232, Reward: -355.3241395505123, Steps: 200\n",
            "Episode: 233, Reward: -260.857177681708, Steps: 200\n",
            "Episode: 234, Reward: -169.0776768982277, Steps: 200\n",
            "Episode: 235, Reward: -350.4796615627085, Steps: 200\n",
            "Episode: 236, Reward: -578.9972771637497, Steps: 200\n",
            "Episode: 237, Reward: -161.81900275880892, Steps: 200\n",
            "Episode: 238, Reward: -541.396479103354, Steps: 200\n",
            "Episode: 239, Reward: -330.2880614280137, Steps: 200\n",
            "Episode: 240, Reward: -639.6367542556131, Steps: 200\n",
            "Episode: 241, Reward: -1033.081040907439, Steps: 200\n",
            "Episode: 242, Reward: -447.220966392589, Steps: 200\n",
            "Episode: 243, Reward: -557.5400561436336, Steps: 200\n",
            "Episode: 244, Reward: -342.1218117006233, Steps: 200\n",
            "Episode: 245, Reward: -584.0862396680131, Steps: 200\n",
            "Episode: 246, Reward: -499.2956842501062, Steps: 200\n",
            "Episode: 247, Reward: -632.9138939587871, Steps: 200\n",
            "Episode: 248, Reward: -368.29673863338115, Steps: 200\n",
            "Episode: 249, Reward: -510.1420745991404, Steps: 200\n",
            "Episode: 250, Reward: -556.7656669152958, Steps: 200\n",
            "Episode: 251, Reward: -438.32558222040404, Steps: 200\n",
            "Episode: 252, Reward: -247.2289079694942, Steps: 200\n",
            "Episode: 253, Reward: -170.8877982642796, Steps: 200\n",
            "Episode: 254, Reward: -66.72063751455154, Steps: 200\n",
            "Episode: 255, Reward: -52.027224549910095, Steps: 200\n",
            "Episode: 256, Reward: -61.566484829514074, Steps: 200\n",
            "Episode: 257, Reward: -70.24852103137135, Steps: 200\n",
            "Episode: 258, Reward: -88.60191635669361, Steps: 200\n",
            "Episode: 259, Reward: -528.6052578612525, Steps: 200\n",
            "Episode: 260, Reward: -39.72493547738828, Steps: 200\n",
            "Episode: 261, Reward: -61.70381481493906, Steps: 200\n",
            "Episode: 262, Reward: -41.4232314127588, Steps: 200\n",
            "Episode: 263, Reward: -66.83141093600614, Steps: 200\n",
            "Episode: 264, Reward: -126.78451029704314, Steps: 200\n",
            "Episode: 265, Reward: -120.3040341282648, Steps: 200\n",
            "Episode: 266, Reward: -49.40308406077729, Steps: 200\n",
            "Episode: 267, Reward: -66.13623515840052, Steps: 200\n",
            "Episode: 268, Reward: -37.42689919071211, Steps: 200\n",
            "Episode: 269, Reward: -145.76103230897388, Steps: 200\n",
            "Episode: 270, Reward: -87.98963631277039, Steps: 200\n",
            "Episode: 271, Reward: -74.43274270818996, Steps: 200\n",
            "Episode: 272, Reward: -265.7963468586032, Steps: 200\n",
            "Episode: 273, Reward: -101.53555797779097, Steps: 200\n",
            "Episode: 274, Reward: -84.30346389814915, Steps: 200\n",
            "Episode: 275, Reward: -55.46724777394505, Steps: 200\n",
            "Episode: 276, Reward: -58.37162217012545, Steps: 200\n",
            "Episode: 277, Reward: -367.79540925978165, Steps: 200\n",
            "Episode: 278, Reward: -58.47332111830427, Steps: 200\n",
            "Episode: 279, Reward: -85.78245099178658, Steps: 200\n",
            "Episode: 280, Reward: -170.5698048101195, Steps: 200\n",
            "Episode: 281, Reward: -86.28317115824187, Steps: 200\n",
            "Episode: 282, Reward: -120.9258269133851, Steps: 200\n",
            "Episode: 283, Reward: -367.9616311622286, Steps: 200\n",
            "Episode: 284, Reward: -167.92197955217577, Steps: 200\n",
            "Episode: 285, Reward: -84.33614688093297, Steps: 200\n",
            "Episode: 286, Reward: -96.90162407971663, Steps: 200\n",
            "Episode: 287, Reward: -126.67993246051529, Steps: 200\n",
            "Episode: 288, Reward: -77.84806381901106, Steps: 200\n",
            "Episode: 289, Reward: -438.27917323852466, Steps: 200\n",
            "Episode: 290, Reward: -443.6634132666833, Steps: 200\n",
            "Episode: 291, Reward: -83.47946411221677, Steps: 200\n",
            "Episode: 292, Reward: -62.64062430891136, Steps: 200\n",
            "Episode: 293, Reward: -397.9396008750275, Steps: 200\n",
            "Episode: 294, Reward: -53.91243264124468, Steps: 200\n",
            "Episode: 295, Reward: -155.02880767312297, Steps: 200\n",
            "Episode: 296, Reward: -58.23415667806754, Steps: 200\n",
            "Episode: 297, Reward: -79.4160644458404, Steps: 200\n",
            "Episode: 298, Reward: -189.24313452286412, Steps: 200\n",
            "Episode: 299, Reward: -257.1192944221958, Steps: 200\n",
            "Episode: 300, Reward: -504.60623793970547, Steps: 200\n",
            "Episode: 301, Reward: -189.6881354436028, Steps: 200\n",
            "Episode: 302, Reward: -84.1847369977915, Steps: 200\n",
            "Episode: 303, Reward: -108.65026709226773, Steps: 200\n",
            "Episode: 304, Reward: -34.81842269243541, Steps: 200\n",
            "Episode: 305, Reward: -162.0726549747128, Steps: 200\n",
            "Episode: 306, Reward: -278.87231635561056, Steps: 200\n",
            "Episode: 307, Reward: -59.69056344957878, Steps: 200\n",
            "Episode: 308, Reward: -48.08255677415299, Steps: 200\n",
            "Episode: 309, Reward: -193.91484207775494, Steps: 200\n",
            "Episode: 310, Reward: -63.33868368422183, Steps: 200\n",
            "Episode: 311, Reward: -52.631752883426245, Steps: 200\n",
            "Episode: 312, Reward: -59.65764400108162, Steps: 200\n",
            "Episode: 313, Reward: -33.175003222717535, Steps: 200\n",
            "Episode: 314, Reward: -322.7428602868074, Steps: 200\n",
            "Episode: 315, Reward: -93.75397014442916, Steps: 200\n",
            "Episode: 316, Reward: -106.62852591126074, Steps: 200\n",
            "Episode: 317, Reward: -46.95368581720362, Steps: 200\n",
            "Episode: 318, Reward: -57.86130150412334, Steps: 200\n",
            "Episode: 319, Reward: -81.6321855040632, Steps: 200\n",
            "Episode: 320, Reward: -76.23278867396438, Steps: 200\n",
            "Episode: 321, Reward: -90.13639075623331, Steps: 200\n",
            "Episode: 322, Reward: -124.13953189849403, Steps: 200\n",
            "Episode: 323, Reward: -39.645509140261666, Steps: 200\n",
            "Episode: 324, Reward: -80.59659014030396, Steps: 200\n",
            "Episode: 325, Reward: -53.96942747244296, Steps: 200\n",
            "Episode: 326, Reward: -68.17252991448291, Steps: 200\n",
            "Episode: 327, Reward: -238.75876743138681, Steps: 200\n",
            "Episode: 328, Reward: -78.78671884664128, Steps: 200\n",
            "Episode: 329, Reward: -283.3062787004163, Steps: 200\n",
            "Episode: 330, Reward: -250.1221756176211, Steps: 200\n",
            "Episode: 331, Reward: -45.44692796948534, Steps: 200\n",
            "Episode: 332, Reward: -298.53786854489647, Steps: 200\n",
            "Episode: 333, Reward: -64.87340340341072, Steps: 200\n",
            "Episode: 334, Reward: -87.94076375627903, Steps: 200\n",
            "Episode: 335, Reward: -113.05435387767251, Steps: 200\n",
            "Episode: 336, Reward: -142.6034751705764, Steps: 200\n",
            "Episode: 337, Reward: -131.98488197920793, Steps: 200\n",
            "Episode: 338, Reward: -89.6246862356128, Steps: 200\n",
            "Episode: 339, Reward: -77.37119297497726, Steps: 200\n",
            "Episode: 340, Reward: -112.91172553171381, Steps: 200\n",
            "Episode: 341, Reward: -80.70024298631462, Steps: 200\n",
            "Episode: 342, Reward: -90.50798427380256, Steps: 200\n",
            "Episode: 343, Reward: -92.21957845452077, Steps: 200\n",
            "Episode: 344, Reward: -78.22030463105696, Steps: 200\n",
            "Episode: 345, Reward: -104.07267745720834, Steps: 200\n",
            "Episode: 346, Reward: -115.89062353955357, Steps: 200\n",
            "Episode: 347, Reward: -519.402359933718, Steps: 200\n",
            "Episode: 348, Reward: -595.4231031635878, Steps: 200\n",
            "Episode: 349, Reward: -52.8773799414193, Steps: 200\n",
            "Episode: 350, Reward: -91.72975277519568, Steps: 200\n",
            "Episode: 351, Reward: -90.17095976079153, Steps: 200\n",
            "Episode: 352, Reward: -38.303560884717626, Steps: 200\n",
            "Episode: 353, Reward: -48.91820455161048, Steps: 200\n",
            "Episode: 354, Reward: -56.330331299575015, Steps: 200\n",
            "Episode: 355, Reward: -44.68680833066211, Steps: 200\n",
            "Episode: 356, Reward: -51.24251571547288, Steps: 200\n",
            "Episode: 357, Reward: -64.14404995728117, Steps: 200\n",
            "Episode: 358, Reward: -50.51510306848171, Steps: 200\n",
            "Episode: 359, Reward: -39.465069933559036, Steps: 200\n",
            "Episode: 360, Reward: -45.780755019660596, Steps: 200\n",
            "Episode: 361, Reward: -46.90926946003397, Steps: 200\n",
            "Episode: 362, Reward: -40.35677144470164, Steps: 200\n",
            "Episode: 363, Reward: -45.11748172068203, Steps: 200\n",
            "Episode: 364, Reward: -137.206518258807, Steps: 200\n",
            "Episode: 365, Reward: -50.47321997489341, Steps: 200\n",
            "Episode: 366, Reward: -62.47168709998547, Steps: 200\n",
            "Episode: 367, Reward: -81.68574047240017, Steps: 200\n",
            "Episode: 368, Reward: -46.61400513564355, Steps: 200\n",
            "Episode: 369, Reward: -31.907016482923822, Steps: 200\n",
            "Episode: 370, Reward: -32.110548116176965, Steps: 200\n",
            "Episode: 371, Reward: -54.79914459912051, Steps: 200\n",
            "Episode: 372, Reward: -24.599193911181022, Steps: 200\n",
            "Episode: 373, Reward: -49.86660350587225, Steps: 200\n",
            "Episode: 374, Reward: -42.70919432475429, Steps: 200\n",
            "Episode: 375, Reward: -37.37624316523851, Steps: 200\n",
            "Episode: 376, Reward: -45.683430121007405, Steps: 200\n",
            "Episode: 377, Reward: -48.93215895510759, Steps: 200\n",
            "Episode: 378, Reward: -52.76647177461166, Steps: 200\n",
            "Episode: 379, Reward: -65.25427826160278, Steps: 200\n",
            "Episode: 380, Reward: -66.01558536705848, Steps: 200\n",
            "Episode: 381, Reward: -131.52345126760403, Steps: 200\n",
            "Episode: 382, Reward: -34.996162750033534, Steps: 200\n",
            "Episode: 383, Reward: -38.58755011452185, Steps: 200\n",
            "Episode: 384, Reward: -47.00209909568702, Steps: 200\n",
            "Episode: 385, Reward: -52.75364739130541, Steps: 200\n",
            "Episode: 386, Reward: -50.162685686785196, Steps: 200\n",
            "Episode: 387, Reward: -44.76673295004134, Steps: 200\n",
            "Episode: 388, Reward: -17.293936946810295, Steps: 200\n",
            "Episode: 389, Reward: -906.1617801777345, Steps: 200\n",
            "Episode: 390, Reward: -34.45480513814542, Steps: 200\n",
            "Episode: 391, Reward: -79.50172411451591, Steps: 200\n",
            "Episode: 392, Reward: -30.080826292855555, Steps: 200\n",
            "Episode: 393, Reward: -47.814672763323834, Steps: 200\n",
            "Episode: 394, Reward: -205.312774700317, Steps: 200\n",
            "Episode: 395, Reward: -133.8986630089169, Steps: 200\n",
            "Episode: 396, Reward: -22.705644322683273, Steps: 200\n",
            "Episode: 397, Reward: -172.88911126232975, Steps: 200\n",
            "Episode: 398, Reward: -44.578051913976445, Steps: 200\n",
            "Episode: 399, Reward: -38.12736627758606, Steps: 200\n",
            "Episode: 400, Reward: -321.4208528574506, Steps: 200\n",
            "Episode: 401, Reward: -934.1911053310969, Steps: 200\n",
            "Episode: 402, Reward: -452.2255499328714, Steps: 200\n",
            "Episode: 403, Reward: -1681.2726818239728, Steps: 200\n",
            "Episode: 404, Reward: -76.46715259349531, Steps: 200\n",
            "Episode: 405, Reward: -87.21857823106023, Steps: 200\n",
            "Episode: 406, Reward: -14.908753038059968, Steps: 200\n",
            "Episode: 407, Reward: -115.79511622455232, Steps: 200\n",
            "Episode: 408, Reward: -787.0149368298107, Steps: 200\n",
            "Episode: 409, Reward: -449.48890117329137, Steps: 200\n",
            "Episode: 410, Reward: -82.67700667490753, Steps: 200\n",
            "Episode: 411, Reward: -30.53777988184994, Steps: 200\n",
            "Episode: 412, Reward: -75.23354450990271, Steps: 200\n",
            "Episode: 413, Reward: -119.01794919307821, Steps: 200\n",
            "Episode: 414, Reward: -54.68462014693212, Steps: 200\n",
            "Episode: 415, Reward: -45.34794132022557, Steps: 200\n",
            "Episode: 416, Reward: -115.9768834475929, Steps: 200\n",
            "Episode: 417, Reward: -94.07372265462837, Steps: 200\n",
            "Episode: 418, Reward: -33.12353340021949, Steps: 200\n",
            "Episode: 419, Reward: -29.386168999703706, Steps: 200\n",
            "Episode: 420, Reward: -52.770435523113186, Steps: 200\n",
            "Episode: 421, Reward: -361.9112838958768, Steps: 200\n",
            "Episode: 422, Reward: -34.58083819128771, Steps: 200\n",
            "Episode: 423, Reward: -51.00880954826071, Steps: 200\n",
            "Episode: 424, Reward: -56.55146969700649, Steps: 200\n",
            "Episode: 425, Reward: -51.72072939486747, Steps: 200\n",
            "Episode: 426, Reward: -28.590948144383926, Steps: 200\n",
            "Episode: 427, Reward: -706.2131130148834, Steps: 200\n",
            "Episode: 428, Reward: -26.644041222823375, Steps: 200\n",
            "Episode: 429, Reward: -37.58345342031385, Steps: 200\n",
            "Episode: 430, Reward: -56.217788351652594, Steps: 200\n",
            "Episode: 431, Reward: -32.29470359213628, Steps: 200\n",
            "Episode: 432, Reward: -61.68818657371614, Steps: 200\n",
            "Episode: 433, Reward: -50.97589023701923, Steps: 200\n",
            "Episode: 434, Reward: -142.98408481824157, Steps: 200\n",
            "Episode: 435, Reward: -306.6946889114477, Steps: 200\n",
            "Episode: 436, Reward: -42.73602570809606, Steps: 200\n",
            "Episode: 437, Reward: -36.49105398309531, Steps: 200\n",
            "Episode: 438, Reward: -23.354552556345165, Steps: 200\n",
            "Episode: 439, Reward: -30.223554130394962, Steps: 200\n",
            "Episode: 440, Reward: -47.06772311506916, Steps: 200\n",
            "Episode: 441, Reward: -50.725382920837674, Steps: 200\n",
            "Episode: 442, Reward: -52.394950665272766, Steps: 200\n",
            "Episode: 443, Reward: -51.32756898231836, Steps: 200\n",
            "Episode: 444, Reward: -36.01853441680543, Steps: 200\n",
            "Episode: 445, Reward: -49.670792057090566, Steps: 200\n",
            "Episode: 446, Reward: -33.3239954852484, Steps: 200\n",
            "Episode: 447, Reward: -166.73854450970424, Steps: 200\n",
            "Episode: 448, Reward: -38.26402165578778, Steps: 200\n",
            "Episode: 449, Reward: -50.62750320627552, Steps: 200\n",
            "Episode: 450, Reward: -76.11442864159737, Steps: 200\n",
            "Episode: 451, Reward: -33.399267858371964, Steps: 200\n",
            "Episode: 452, Reward: -14.777880618525591, Steps: 200\n",
            "Episode: 453, Reward: -21.668684132222566, Steps: 200\n",
            "Episode: 454, Reward: -31.025387323607617, Steps: 200\n",
            "Episode: 455, Reward: -23.83921001749023, Steps: 200\n",
            "Episode: 456, Reward: -216.7083235937694, Steps: 200\n",
            "Episode: 457, Reward: -476.92580570839016, Steps: 200\n",
            "Episode: 458, Reward: -28.29803836480573, Steps: 200\n",
            "Episode: 459, Reward: -14.267980242782247, Steps: 200\n",
            "Episode: 460, Reward: -16.615483684371938, Steps: 200\n",
            "Episode: 461, Reward: -22.86931025276872, Steps: 200\n",
            "Episode: 462, Reward: -21.56054535042144, Steps: 200\n",
            "Episode: 463, Reward: -28.646056492357182, Steps: 200\n",
            "Episode: 464, Reward: -26.140256699894593, Steps: 200\n",
            "Episode: 465, Reward: -37.14531798549419, Steps: 200\n",
            "Episode: 466, Reward: -12.397797074196445, Steps: 200\n",
            "Episode: 467, Reward: -27.72423848488833, Steps: 200\n",
            "Episode: 468, Reward: -27.65847400402415, Steps: 200\n",
            "Episode: 469, Reward: -39.356346180797196, Steps: 200\n",
            "Episode: 470, Reward: -21.37216365778824, Steps: 200\n",
            "Episode: 471, Reward: -18.981276982377253, Steps: 200\n",
            "Episode: 472, Reward: -26.026171330575906, Steps: 200\n",
            "Episode: 473, Reward: -27.675909042844243, Steps: 200\n",
            "Episode: 474, Reward: -20.890595533569833, Steps: 200\n",
            "Episode: 475, Reward: -33.15730195354681, Steps: 200\n",
            "Episode: 476, Reward: -35.61606754397727, Steps: 200\n",
            "Episode: 477, Reward: -33.63297323216723, Steps: 200\n",
            "Episode: 478, Reward: -26.539253484643996, Steps: 200\n",
            "Episode: 479, Reward: -19.21938304841777, Steps: 200\n",
            "Episode: 480, Reward: -16.06034075514919, Steps: 200\n",
            "Episode: 481, Reward: -32.41400300953664, Steps: 200\n",
            "Episode: 482, Reward: -49.33449450463066, Steps: 200\n",
            "Episode: 483, Reward: -25.533965588339456, Steps: 200\n",
            "Episode: 484, Reward: -23.837630971124494, Steps: 200\n",
            "Episode: 485, Reward: -23.31547826487978, Steps: 200\n",
            "Episode: 486, Reward: -90.24876466762393, Steps: 200\n",
            "Episode: 487, Reward: -24.411463496756493, Steps: 200\n",
            "Episode: 488, Reward: -14.398734459669516, Steps: 200\n",
            "Episode: 489, Reward: -17.89039781634786, Steps: 200\n",
            "Episode: 490, Reward: -22.211212088890125, Steps: 200\n",
            "Episode: 491, Reward: -20.98497808174885, Steps: 200\n",
            "Episode: 492, Reward: -15.722525781098753, Steps: 200\n",
            "Episode: 493, Reward: -23.094344350509004, Steps: 200\n",
            "Episode: 494, Reward: -18.52336394789917, Steps: 200\n",
            "Episode: 495, Reward: -20.282479352147444, Steps: 200\n",
            "Episode: 496, Reward: -17.40373341443248, Steps: 200\n",
            "Episode: 497, Reward: -25.98018488490163, Steps: 200\n",
            "Episode: 498, Reward: -18.770106850482744, Steps: 200\n",
            "Episode: 499, Reward: -18.74705896130067, Steps: 200\n",
            "Episode: 500, Reward: -16.712077042143235, Steps: 200\n",
            "Episode: 501, Reward: -11.156006333154272, Steps: 200\n",
            "Episode: 502, Reward: -19.142299330425683, Steps: 200\n",
            "Episode: 503, Reward: -14.076598531778451, Steps: 200\n",
            "Episode: 504, Reward: -15.234265821409688, Steps: 200\n",
            "Episode: 505, Reward: -25.112582585611673, Steps: 200\n",
            "Episode: 506, Reward: -15.090982181191816, Steps: 200\n",
            "Episode: 507, Reward: -17.18989023918237, Steps: 200\n",
            "Episode: 508, Reward: -34.26756319155699, Steps: 200\n",
            "Episode: 509, Reward: -47.200972610000065, Steps: 200\n",
            "Episode: 510, Reward: -19.74793575900217, Steps: 200\n",
            "Episode: 511, Reward: -8.894405892163014, Steps: 200\n",
            "Episode: 512, Reward: -11.134612537616048, Steps: 200\n",
            "Episode: 513, Reward: -7.7715288649274195, Steps: 200\n",
            "Episode: 514, Reward: -18.06972156417154, Steps: 200\n",
            "Episode: 515, Reward: -15.817480683312201, Steps: 200\n",
            "Episode: 516, Reward: -562.8381811769391, Steps: 200\n",
            "Episode: 517, Reward: -10.864747421793007, Steps: 200\n",
            "Episode: 518, Reward: -14.28122935583569, Steps: 200\n",
            "Episode: 519, Reward: -20.4490478399801, Steps: 200\n",
            "Episode: 520, Reward: -54.976675983952575, Steps: 200\n",
            "Episode: 521, Reward: -12.621648493494636, Steps: 200\n",
            "Episode: 522, Reward: -16.217159912466606, Steps: 200\n",
            "Episode: 523, Reward: -16.22609160249447, Steps: 200\n",
            "Episode: 524, Reward: -9.693444449685323, Steps: 200\n",
            "Episode: 525, Reward: -26.61349192505576, Steps: 200\n",
            "Episode: 526, Reward: -14.359060964982964, Steps: 200\n",
            "Episode: 527, Reward: -16.309720604729243, Steps: 200\n",
            "Episode: 528, Reward: -8.832857804818996, Steps: 200\n",
            "Episode: 529, Reward: -11.354092175259156, Steps: 200\n",
            "Episode: 530, Reward: -28.64076395191118, Steps: 200\n",
            "Episode: 531, Reward: -12.21703589935802, Steps: 200\n",
            "Episode: 532, Reward: -19.209993294386454, Steps: 200\n",
            "Episode: 533, Reward: -18.063533495584426, Steps: 200\n",
            "Episode: 534, Reward: -39.511440770709974, Steps: 200\n",
            "Episode: 535, Reward: -27.92807049933401, Steps: 200\n",
            "Episode: 536, Reward: -13.49634165253146, Steps: 200\n",
            "Episode: 537, Reward: -15.065499968632045, Steps: 200\n",
            "Episode: 538, Reward: -11.02281632350152, Steps: 200\n",
            "Episode: 539, Reward: -16.034737565672106, Steps: 200\n",
            "Episode: 540, Reward: -66.21423808849624, Steps: 200\n",
            "Episode: 541, Reward: -19.67943548379029, Steps: 200\n",
            "Episode: 542, Reward: -15.244796278913634, Steps: 200\n",
            "Episode: 543, Reward: -10.526662310737812, Steps: 200\n",
            "Episode: 544, Reward: -591.4691583779687, Steps: 200\n",
            "Episode: 545, Reward: -19.53663112916346, Steps: 200\n",
            "Episode: 546, Reward: -12.35003832911478, Steps: 200\n",
            "Episode: 547, Reward: -21.33943105658934, Steps: 200\n",
            "Episode: 548, Reward: -9.595457589868202, Steps: 200\n",
            "Episode: 549, Reward: -21.33309738282068, Steps: 200\n",
            "Episode: 550, Reward: -12.908025437496619, Steps: 200\n",
            "Episode: 551, Reward: -14.743219940942339, Steps: 200\n",
            "Episode: 552, Reward: -15.791391274234291, Steps: 200\n",
            "Episode: 553, Reward: -8.15801413714254, Steps: 200\n",
            "Episode: 554, Reward: -26.01551884381204, Steps: 200\n",
            "Episode: 555, Reward: -14.634117982045504, Steps: 200\n",
            "Episode: 556, Reward: -5.043805079729239, Steps: 200\n",
            "Episode: 557, Reward: -8.910530843059608, Steps: 200\n",
            "Episode: 558, Reward: -15.788136613004204, Steps: 200\n",
            "Episode: 559, Reward: -9.071730637945743, Steps: 200\n",
            "Episode: 560, Reward: -9.533463467689998, Steps: 200\n",
            "Episode: 561, Reward: -15.270257154044058, Steps: 200\n",
            "Episode: 562, Reward: -13.389157612075241, Steps: 200\n",
            "Episode: 563, Reward: -12.854747893206921, Steps: 200\n",
            "Episode: 564, Reward: -21.166015396794197, Steps: 200\n",
            "Episode: 565, Reward: -22.461785711909574, Steps: 200\n",
            "Episode: 566, Reward: -16.374025937235196, Steps: 200\n",
            "Episode: 567, Reward: -28.80408870193611, Steps: 200\n",
            "Episode: 568, Reward: -22.279201901923972, Steps: 200\n",
            "Episode: 569, Reward: -6.931805297912451, Steps: 200\n",
            "Episode: 570, Reward: -16.032905607749232, Steps: 200\n",
            "Episode: 571, Reward: -10.071426073175912, Steps: 200\n",
            "Episode: 572, Reward: -15.411079109703667, Steps: 200\n",
            "Episode: 573, Reward: -11.218016982643979, Steps: 200\n",
            "Episode: 574, Reward: -7.147391693890734, Steps: 200\n",
            "Episode: 575, Reward: -5.681996242934624, Steps: 200\n",
            "Episode: 576, Reward: -8.955381136386798, Steps: 200\n",
            "Episode: 577, Reward: -348.483489834097, Steps: 200\n",
            "Episode: 578, Reward: -24.163725711798048, Steps: 200\n",
            "Episode: 579, Reward: -7.150813397307297, Steps: 200\n",
            "Episode: 580, Reward: -6.404737106472427, Steps: 200\n",
            "Episode: 581, Reward: -19.743984747383582, Steps: 200\n",
            "Episode: 582, Reward: -12.45240796632894, Steps: 200\n",
            "Episode: 583, Reward: -9.653082124731567, Steps: 200\n",
            "Episode: 584, Reward: -12.215611538805112, Steps: 200\n",
            "Episode: 585, Reward: -7.320039274535194, Steps: 200\n",
            "Episode: 586, Reward: -5.831024107028218, Steps: 200\n",
            "Episode: 587, Reward: -10.319326094033986, Steps: 200\n",
            "Episode: 588, Reward: -8.571138178240462, Steps: 200\n",
            "Episode: 589, Reward: -6.44807243121779, Steps: 200\n",
            "Episode: 590, Reward: -17.709636904311907, Steps: 200\n",
            "Episode: 591, Reward: -7.291435374719151, Steps: 200\n",
            "Episode: 592, Reward: -9.443392656547223, Steps: 200\n",
            "Episode: 593, Reward: -11.889662550951437, Steps: 200\n",
            "Episode: 594, Reward: -6.946751844094587, Steps: 200\n",
            "Episode: 595, Reward: -9.865784795284224, Steps: 200\n",
            "Episode: 596, Reward: -6.775157903553925, Steps: 200\n",
            "Episode: 597, Reward: -25.15360799447289, Steps: 200\n",
            "Episode: 598, Reward: -7.868000611256492, Steps: 200\n",
            "Episode: 599, Reward: -6.643351979036074, Steps: 200\n",
            "Episode: 600, Reward: -6.048495130040105, Steps: 200\n",
            "Episode: 601, Reward: -21.7936015621354, Steps: 200\n",
            "Episode: 602, Reward: -23.367057356995012, Steps: 200\n",
            "Episode: 603, Reward: -4.718472915171012, Steps: 200\n",
            "Episode: 604, Reward: -3.8920045397722007, Steps: 200\n",
            "Episode: 605, Reward: -13.157515283241711, Steps: 200\n",
            "Episode: 606, Reward: -117.86993248142078, Steps: 200\n",
            "Episode: 607, Reward: -87.12405072977806, Steps: 200\n",
            "Episode: 608, Reward: -14.915790137829807, Steps: 200\n",
            "Episode: 609, Reward: -10.706800256537768, Steps: 200\n",
            "Episode: 610, Reward: -26.706608720705308, Steps: 200\n",
            "Episode: 611, Reward: -9.098427477554049, Steps: 200\n",
            "Episode: 612, Reward: -27.797574576676006, Steps: 200\n",
            "Episode: 613, Reward: -21.40778563736501, Steps: 200\n",
            "Episode: 614, Reward: -9.478677680134874, Steps: 200\n",
            "Episode: 615, Reward: -7.4380365358626745, Steps: 200\n",
            "Episode: 616, Reward: -7.772450771839341, Steps: 200\n",
            "Episode: 617, Reward: -10.213100083002063, Steps: 200\n",
            "Episode: 618, Reward: -13.27163274982065, Steps: 200\n",
            "Episode: 619, Reward: -7.534021802237212, Steps: 200\n",
            "Episode: 620, Reward: -22.73297586365938, Steps: 200\n",
            "Episode: 621, Reward: -4.63990463801948, Steps: 200\n",
            "Episode: 622, Reward: -3.231265334985594, Steps: 200\n",
            "Episode: 623, Reward: -16.42628900652906, Steps: 200\n",
            "Episode: 624, Reward: -16.282114396328833, Steps: 200\n",
            "Episode: 625, Reward: -7.139565642597768, Steps: 200\n",
            "Episode: 626, Reward: -7.456371810809339, Steps: 200\n",
            "Episode: 627, Reward: -41.01966459932646, Steps: 200\n",
            "Episode: 628, Reward: -5.744837429107241, Steps: 200\n",
            "Episode: 629, Reward: -2.953388872799486, Steps: 200\n",
            "Episode: 630, Reward: -7.599114042746793, Steps: 200\n",
            "Episode: 631, Reward: -9.966046680910956, Steps: 200\n",
            "Episode: 632, Reward: -34.82914427996528, Steps: 200\n",
            "Episode: 633, Reward: -4.407047737459649, Steps: 200\n",
            "Episode: 634, Reward: -20.20931816036639, Steps: 200\n",
            "Episode: 635, Reward: -5.394404374926279, Steps: 200\n",
            "Episode: 636, Reward: -9.900509225398164, Steps: 200\n",
            "Episode: 637, Reward: -8.537715136214704, Steps: 200\n",
            "Episode: 638, Reward: -11.632480528031333, Steps: 200\n",
            "Episode: 639, Reward: -16.78198794564889, Steps: 200\n",
            "Episode: 640, Reward: -13.020296362039947, Steps: 200\n",
            "Episode: 641, Reward: -33.56970632719818, Steps: 200\n",
            "Episode: 642, Reward: -14.811549358266545, Steps: 200\n",
            "Episode: 643, Reward: -19.76438231466961, Steps: 200\n",
            "Episode: 644, Reward: -5.026364805436615, Steps: 200\n",
            "Episode: 645, Reward: -12.418594866788615, Steps: 200\n",
            "Episode: 646, Reward: -3.535217642893082, Steps: 200\n",
            "Episode: 647, Reward: -27.404739119050387, Steps: 200\n",
            "Episode: 648, Reward: -106.01930752766478, Steps: 200\n",
            "Episode: 649, Reward: -5.780682880426872, Steps: 200\n",
            "Episode: 650, Reward: -7.609465486425501, Steps: 200\n",
            "Episode: 651, Reward: -9.49855564596821, Steps: 200\n",
            "Episode: 652, Reward: -3.466761110024376, Steps: 200\n",
            "Episode: 653, Reward: -7.7217171421586475, Steps: 200\n",
            "Episode: 654, Reward: -194.7096271362808, Steps: 200\n",
            "Episode: 655, Reward: -7.262100762047835, Steps: 200\n",
            "Episode: 656, Reward: -542.646361899868, Steps: 200\n",
            "Episode: 657, Reward: -136.7923225708037, Steps: 200\n",
            "Episode: 658, Reward: -17.58265219566836, Steps: 200\n",
            "Episode: 659, Reward: -13.930369044165708, Steps: 200\n",
            "Episode: 660, Reward: -2.0250832575772506, Steps: 200\n",
            "Episode: 661, Reward: -8.601809794603193, Steps: 200\n",
            "Episode: 662, Reward: -4.987703931237879, Steps: 200\n",
            "Episode: 663, Reward: -7.623246009879559, Steps: 200\n",
            "Episode: 664, Reward: -29.09811071106028, Steps: 200\n",
            "Episode: 665, Reward: -389.7651146082763, Steps: 200\n",
            "Episode: 666, Reward: -11.879981087447815, Steps: 200\n",
            "Episode: 667, Reward: -8.190530053286869, Steps: 200\n",
            "Episode: 668, Reward: -9.515104407907295, Steps: 200\n",
            "Episode: 669, Reward: -10.953196914857662, Steps: 200\n",
            "Episode: 670, Reward: -13.417222639434684, Steps: 200\n",
            "Episode: 671, Reward: -10.581463690594976, Steps: 200\n",
            "Episode: 672, Reward: -5.0838801320218545, Steps: 200\n",
            "Episode: 673, Reward: -8.167089929404963, Steps: 200\n",
            "Episode: 674, Reward: -18.207501181716104, Steps: 200\n",
            "Episode: 675, Reward: -10.391127107533807, Steps: 200\n",
            "Episode: 676, Reward: -7.679552463907515, Steps: 200\n",
            "Episode: 677, Reward: -5.16108674766893, Steps: 200\n",
            "Episode: 678, Reward: -3.344050320057445, Steps: 200\n",
            "Episode: 679, Reward: -9.917163999939506, Steps: 200\n",
            "Episode: 680, Reward: -3.1636209905419115, Steps: 200\n",
            "Episode: 681, Reward: -12.994824560377033, Steps: 200\n",
            "Episode: 682, Reward: -6.718528889880392, Steps: 200\n",
            "Episode: 683, Reward: -3.081191394102999, Steps: 200\n",
            "Episode: 684, Reward: -5.834574831987402, Steps: 200\n",
            "Episode: 685, Reward: -9.980174909054748, Steps: 200\n",
            "Episode: 686, Reward: -4.922022134053205, Steps: 200\n",
            "Episode: 687, Reward: -4.629897470661609, Steps: 200\n",
            "Episode: 688, Reward: -335.2481974230534, Steps: 200\n",
            "Episode: 689, Reward: -4.983080350073939, Steps: 200\n",
            "Episode: 690, Reward: -11.252921014811347, Steps: 200\n",
            "Episode: 691, Reward: -70.51308398285252, Steps: 200\n",
            "Episode: 692, Reward: -5.070080495584463, Steps: 200\n",
            "Episode: 693, Reward: -100.79162224767933, Steps: 200\n",
            "Episode: 694, Reward: -16.01434870522615, Steps: 200\n",
            "Episode: 695, Reward: -13.166441648293024, Steps: 200\n",
            "Episode: 696, Reward: -4.427804383489541, Steps: 200\n",
            "Episode: 697, Reward: -7.9426958049737655, Steps: 200\n",
            "Episode: 698, Reward: -28.072444026240724, Steps: 200\n",
            "Episode: 699, Reward: -4.654548656076479, Steps: 200\n",
            "Episode: 700, Reward: -7.533092979928752, Steps: 200\n",
            "Episode: 701, Reward: -4.792016526348018, Steps: 200\n",
            "Episode: 702, Reward: -8.985506629334644, Steps: 200\n",
            "Episode: 703, Reward: -4.127236468904309, Steps: 200\n",
            "Episode: 704, Reward: -4.876923368693788, Steps: 200\n",
            "Episode: 705, Reward: -46.669007641748834, Steps: 200\n",
            "Episode: 706, Reward: -17.784255415370367, Steps: 200\n",
            "Episode: 707, Reward: -4.684448690303862, Steps: 200\n",
            "Episode: 708, Reward: -21.031782449824924, Steps: 200\n",
            "Episode: 709, Reward: -2.885759169133313, Steps: 200\n",
            "Episode: 710, Reward: -7.633619789348816, Steps: 200\n",
            "Episode: 711, Reward: -13.361088722351521, Steps: 200\n",
            "Episode: 712, Reward: -3.6993075833542255, Steps: 200\n",
            "Episode: 713, Reward: -8.91955409482317, Steps: 200\n",
            "Episode: 714, Reward: -8.295779724106259, Steps: 200\n",
            "Episode: 715, Reward: -6.64298308645717, Steps: 200\n",
            "Episode: 716, Reward: -43.167901884517455, Steps: 200\n",
            "Episode: 717, Reward: -7.485074803366876, Steps: 200\n",
            "Episode: 718, Reward: -17.42950357555311, Steps: 200\n",
            "Episode: 719, Reward: -4.17729034936597, Steps: 200\n",
            "Episode: 720, Reward: -6.407698122706866, Steps: 200\n",
            "Episode: 721, Reward: -7.592019919541467, Steps: 200\n",
            "Episode: 722, Reward: -15.820922179805205, Steps: 200\n",
            "Episode: 723, Reward: -4.5066666884119, Steps: 200\n",
            "Episode: 724, Reward: -9.6964691744998, Steps: 200\n",
            "Episode: 725, Reward: -18.599163123714657, Steps: 200\n",
            "Episode: 726, Reward: -3.2509882593848136, Steps: 200\n",
            "Episode: 727, Reward: -10.104075864123603, Steps: 200\n",
            "Episode: 728, Reward: -5.873590388279507, Steps: 200\n",
            "Episode: 729, Reward: -2.8346434984261952, Steps: 200\n",
            "Episode: 730, Reward: -5.855089704332847, Steps: 200\n",
            "Episode: 731, Reward: -4.233467758835325, Steps: 200\n",
            "Episode: 732, Reward: -3.7395723448030034, Steps: 200\n",
            "Episode: 733, Reward: -10.647694280209338, Steps: 200\n",
            "Episode: 734, Reward: -2.5342938494302283, Steps: 200\n",
            "Episode: 735, Reward: -7.7735695005813925, Steps: 200\n",
            "Episode: 736, Reward: -254.86177070475543, Steps: 200\n",
            "Episode: 737, Reward: -13.10000170610488, Steps: 200\n",
            "Episode: 738, Reward: -9.05852609608796, Steps: 200\n",
            "Episode: 739, Reward: -15.619569241368607, Steps: 200\n",
            "Episode: 740, Reward: -10.522295444487044, Steps: 200\n",
            "Episode: 741, Reward: -6.751044600965215, Steps: 200\n",
            "Episode: 742, Reward: -3.5242547916514066, Steps: 200\n",
            "Episode: 743, Reward: -17.956663938434225, Steps: 200\n",
            "Episode: 744, Reward: -5.648792538264731, Steps: 200\n",
            "Episode: 745, Reward: -4.612832043893576, Steps: 200\n",
            "Episode: 746, Reward: -6.472027851637403, Steps: 200\n",
            "Episode: 747, Reward: -4.707582403761879, Steps: 200\n",
            "Episode: 748, Reward: -3.7844251855232978, Steps: 200\n",
            "Episode: 749, Reward: -13.853058889852058, Steps: 200\n",
            "Episode: 750, Reward: -9.07738897508562, Steps: 200\n",
            "Episode: 751, Reward: -3.942947289906051, Steps: 200\n",
            "Episode: 752, Reward: -9.478459580614894, Steps: 200\n",
            "Episode: 753, Reward: -2.431684886273563, Steps: 200\n",
            "Episode: 754, Reward: -4.224941768269445, Steps: 200\n",
            "Episode: 755, Reward: -6.778472337617764, Steps: 200\n",
            "Episode: 756, Reward: -1.9644937448668165, Steps: 200\n",
            "Episode: 757, Reward: -5.504066831614739, Steps: 200\n",
            "Episode: 758, Reward: -15.827764391371206, Steps: 200\n",
            "Episode: 759, Reward: -5.261070346313169, Steps: 200\n",
            "Episode: 760, Reward: -4.166233772280893, Steps: 200\n",
            "Episode: 761, Reward: -9.300465365481925, Steps: 200\n",
            "Episode: 762, Reward: -710.088060612615, Steps: 200\n",
            "Episode: 763, Reward: -17.905947374422304, Steps: 200\n",
            "Episode: 764, Reward: -3.2752731988534007, Steps: 200\n",
            "Episode: 765, Reward: -4.96678095142198, Steps: 200\n",
            "Episode: 766, Reward: -5.218292923855976, Steps: 200\n",
            "Episode: 767, Reward: -4.4070026497615915, Steps: 200\n",
            "Episode: 768, Reward: -2.7487488573041765, Steps: 200\n",
            "Episode: 769, Reward: -4.190949954535842, Steps: 200\n",
            "Episode: 770, Reward: -1.9843828014118172, Steps: 200\n",
            "Episode: 771, Reward: -4.05046503166017, Steps: 200\n",
            "Episode: 772, Reward: -14.682018861225295, Steps: 200\n",
            "Episode: 773, Reward: -2.825495159513167, Steps: 200\n",
            "Episode: 774, Reward: -59.021866635924034, Steps: 200\n",
            "Episode: 775, Reward: -2.9925768232986893, Steps: 200\n",
            "Episode: 776, Reward: -5.99016631295818, Steps: 200\n",
            "Episode: 777, Reward: -12.185471791776923, Steps: 200\n",
            "Episode: 778, Reward: -4.159687488434445, Steps: 200\n",
            "Episode: 779, Reward: -10.882294573841266, Steps: 200\n",
            "Episode: 780, Reward: -10.777589077174074, Steps: 200\n",
            "Episode: 781, Reward: -4.839305799982956, Steps: 200\n",
            "Episode: 782, Reward: -936.7889058219921, Steps: 200\n",
            "Episode: 783, Reward: -4.922335987333946, Steps: 200\n",
            "Episode: 784, Reward: -11.342276981778424, Steps: 200\n",
            "Episode: 785, Reward: -5.812005752434249, Steps: 200\n",
            "Episode: 786, Reward: -21.344900035653712, Steps: 200\n",
            "Episode: 787, Reward: -18.94445241254337, Steps: 200\n",
            "Episode: 788, Reward: -3.4417843559887658, Steps: 200\n",
            "Episode: 789, Reward: -10.830628376039545, Steps: 200\n",
            "Episode: 790, Reward: -16.258007395419607, Steps: 200\n",
            "Episode: 791, Reward: -6.537147636756128, Steps: 200\n",
            "Episode: 792, Reward: -28.28403547764544, Steps: 200\n",
            "Episode: 793, Reward: -98.26507444968733, Steps: 200\n",
            "Episode: 794, Reward: -27.0826976604644, Steps: 200\n",
            "Episode: 795, Reward: -9.008882634033851, Steps: 200\n",
            "Episode: 796, Reward: -16.67728022651593, Steps: 200\n",
            "Episode: 797, Reward: -16.674848100042105, Steps: 200\n",
            "Episode: 798, Reward: -4.363163605113249, Steps: 200\n",
            "Episode: 799, Reward: -4.1738426968236055, Steps: 200\n",
            "Episode: 800, Reward: -4.888666356745833, Steps: 200\n",
            "Episode: 801, Reward: -10.930474380622398, Steps: 200\n",
            "Episode: 802, Reward: -7.244702773475962, Steps: 200\n",
            "Episode: 803, Reward: -10.726680067642903, Steps: 200\n",
            "Episode: 804, Reward: -237.30920816444225, Steps: 200\n",
            "Episode: 805, Reward: -6.601303098429826, Steps: 200\n",
            "Episode: 806, Reward: -8.308262223748082, Steps: 200\n",
            "Episode: 807, Reward: -6.539962893117885, Steps: 200\n",
            "Episode: 808, Reward: -23.13987594885704, Steps: 200\n",
            "Episode: 809, Reward: -205.84299794139284, Steps: 200\n",
            "Episode: 810, Reward: -4.291025532079339, Steps: 200\n",
            "Episode: 811, Reward: -51.11536291329849, Steps: 200\n",
            "Episode: 812, Reward: -521.3076767283113, Steps: 200\n",
            "Episode: 813, Reward: -23.1806428554638, Steps: 200\n",
            "Episode: 814, Reward: -8.477066066419123, Steps: 200\n",
            "Episode: 815, Reward: -35.46479845293881, Steps: 200\n",
            "Episode: 816, Reward: -16.82810940233458, Steps: 200\n",
            "Episode: 817, Reward: -35.31411693700179, Steps: 200\n",
            "Episode: 818, Reward: -6.4923105578346645, Steps: 200\n",
            "Episode: 819, Reward: -12.59235339680358, Steps: 200\n",
            "Episode: 820, Reward: -11.211463634579241, Steps: 200\n",
            "Episode: 821, Reward: -5.010924636798239, Steps: 200\n",
            "Episode: 822, Reward: -6.267969563344428, Steps: 200\n",
            "Episode: 823, Reward: -14.859240902829683, Steps: 200\n",
            "Episode: 824, Reward: -25.200638284531646, Steps: 200\n",
            "Episode: 825, Reward: -6.2275394880832895, Steps: 200\n",
            "Episode: 826, Reward: -10.493823893633639, Steps: 200\n",
            "Episode: 827, Reward: -15.837993699587914, Steps: 200\n",
            "Episode: 828, Reward: -10.635451987992484, Steps: 200\n",
            "Episode: 829, Reward: -10.416025147303095, Steps: 200\n",
            "Episode: 830, Reward: -323.8265096365631, Steps: 200\n",
            "Episode: 831, Reward: -10.342905406866159, Steps: 200\n",
            "Episode: 832, Reward: -711.113662183867, Steps: 200\n",
            "Episode: 833, Reward: -9.381899808432454, Steps: 200\n",
            "Episode: 834, Reward: -9.89068688978135, Steps: 200\n",
            "Episode: 835, Reward: -9.044074433188399, Steps: 200\n",
            "Episode: 836, Reward: -2.471437030175144, Steps: 200\n",
            "Episode: 837, Reward: -264.2137879995868, Steps: 200\n",
            "Episode: 838, Reward: -317.2539434657449, Steps: 200\n",
            "Episode: 839, Reward: -7.8755575780756075, Steps: 200\n",
            "Episode: 840, Reward: -4.513355425897378, Steps: 200\n",
            "Episode: 841, Reward: -2.1894066533781986, Steps: 200\n",
            "Episode: 842, Reward: -3.6106920323145006, Steps: 200\n",
            "Episode: 843, Reward: -11.398502109359645, Steps: 200\n",
            "Episode: 844, Reward: -75.73049927741965, Steps: 200\n",
            "Episode: 845, Reward: -5.0002786008170474, Steps: 200\n",
            "Episode: 846, Reward: -12.726096111276542, Steps: 200\n",
            "Episode: 847, Reward: -12.136403724246575, Steps: 200\n",
            "Episode: 848, Reward: -5.5194199587921515, Steps: 200\n",
            "Episode: 849, Reward: -4.669584294345519, Steps: 200\n",
            "Episode: 850, Reward: -3.6717717650707193, Steps: 200\n",
            "Episode: 851, Reward: -354.67558930485706, Steps: 200\n",
            "Episode: 852, Reward: -8.249475580296854, Steps: 200\n",
            "Episode: 853, Reward: -461.653438996896, Steps: 200\n",
            "Episode: 854, Reward: -13.237453770176835, Steps: 200\n",
            "Episode: 855, Reward: -580.9992169818183, Steps: 200\n",
            "Episode: 856, Reward: -16.574121093595494, Steps: 200\n",
            "Episode: 857, Reward: -5.008419868565021, Steps: 200\n",
            "Episode: 858, Reward: -6.014531048384481, Steps: 200\n",
            "Episode: 859, Reward: -24.375910614890444, Steps: 200\n",
            "Episode: 860, Reward: -4.902732949175752, Steps: 200\n",
            "Episode: 861, Reward: -3.417312756385517, Steps: 200\n",
            "Episode: 862, Reward: -5.056842635417539, Steps: 200\n",
            "Episode: 863, Reward: -3.329806629904036, Steps: 200\n",
            "Episode: 864, Reward: -3.2414681410382613, Steps: 200\n",
            "Episode: 865, Reward: -24.37167129907228, Steps: 200\n",
            "Episode: 866, Reward: -51.88606569332077, Steps: 200\n",
            "Episode: 867, Reward: -4.075470789846539, Steps: 200\n",
            "Episode: 868, Reward: -3.418723207460942, Steps: 200\n",
            "Episode: 869, Reward: -12.84769502377992, Steps: 200\n",
            "Episode: 870, Reward: -5.459276549706184, Steps: 200\n",
            "Episode: 871, Reward: -4.0136549499551295, Steps: 200\n",
            "Episode: 872, Reward: -5.18686136204048, Steps: 200\n",
            "Episode: 873, Reward: -10.773366535130252, Steps: 200\n",
            "Episode: 874, Reward: -3.0151168268385145, Steps: 200\n",
            "Episode: 875, Reward: -37.95268554049121, Steps: 200\n",
            "Episode: 876, Reward: -19.682482255894605, Steps: 200\n",
            "Episode: 877, Reward: -4.3120408031886575, Steps: 200\n",
            "Episode: 878, Reward: -10.47554293713106, Steps: 200\n",
            "Episode: 879, Reward: -4.672881764681611, Steps: 200\n",
            "Episode: 880, Reward: -3.3292965574889997, Steps: 200\n",
            "Episode: 881, Reward: -61.704015858276826, Steps: 200\n",
            "Episode: 882, Reward: -16.798677427817537, Steps: 200\n",
            "Episode: 883, Reward: -9.97677246725218, Steps: 200\n",
            "Episode: 884, Reward: -3.3945883341077905, Steps: 200\n",
            "Episode: 885, Reward: -4.035412915244269, Steps: 200\n",
            "Episode: 886, Reward: -2.8360042813025395, Steps: 200\n",
            "Episode: 887, Reward: -16.22661824400317, Steps: 200\n",
            "Episode: 888, Reward: -8.940115742565096, Steps: 200\n",
            "Episode: 889, Reward: -2.4678342483676907, Steps: 200\n",
            "Episode: 890, Reward: -8.833946948123899, Steps: 200\n",
            "Episode: 891, Reward: -5.401321183193748, Steps: 200\n",
            "Episode: 892, Reward: -6.589695045146005, Steps: 200\n",
            "Episode: 893, Reward: -3.5243519398102645, Steps: 200\n",
            "Episode: 894, Reward: -6.752073980786319, Steps: 200\n",
            "Episode: 895, Reward: -4.229745385481269, Steps: 200\n",
            "Episode: 896, Reward: -3.39969249152594, Steps: 200\n",
            "Episode: 897, Reward: -13.816431926215948, Steps: 200\n",
            "Episode: 898, Reward: -23.115344124588237, Steps: 200\n",
            "Episode: 899, Reward: -7.870960056283257, Steps: 200\n",
            "Episode: 900, Reward: -3.045384398415381, Steps: 200\n",
            "Episode: 901, Reward: -4.297214752031433, Steps: 200\n",
            "Episode: 902, Reward: -6.178116698820963, Steps: 200\n",
            "Episode: 903, Reward: -5.153020215307236, Steps: 200\n",
            "Episode: 904, Reward: -5.924952561570558, Steps: 200\n",
            "Episode: 905, Reward: -1.821720838223075, Steps: 200\n",
            "Episode: 906, Reward: -4.072999942033986, Steps: 200\n",
            "Episode: 907, Reward: -6.917172867750639, Steps: 200\n",
            "Episode: 908, Reward: -5.843491378688142, Steps: 200\n",
            "Episode: 909, Reward: -6.08742337024636, Steps: 200\n",
            "Episode: 910, Reward: -4.189435799399019, Steps: 200\n",
            "Episode: 911, Reward: -3.079344183453962, Steps: 200\n",
            "Episode: 912, Reward: -4.829673897851505, Steps: 200\n",
            "Episode: 913, Reward: -2.512426021798883, Steps: 200\n",
            "Episode: 914, Reward: -3.2496200117291663, Steps: 200\n",
            "Episode: 915, Reward: -4.0435908700639684, Steps: 200\n",
            "Episode: 916, Reward: -6.921429065298448, Steps: 200\n",
            "Episode: 917, Reward: -2.1962424178436897, Steps: 200\n",
            "Episode: 918, Reward: -5.382934176762856, Steps: 200\n",
            "Episode: 919, Reward: -17.50328345785736, Steps: 200\n",
            "Episode: 920, Reward: -2.26616134557466, Steps: 200\n",
            "Episode: 921, Reward: -4.726980476332792, Steps: 200\n",
            "Episode: 922, Reward: -713.8538114667028, Steps: 200\n",
            "Episode: 923, Reward: -10.004747949443113, Steps: 200\n",
            "Episode: 924, Reward: -13.842915286203656, Steps: 200\n",
            "Episode: 925, Reward: -19.8381288380317, Steps: 200\n",
            "Episode: 926, Reward: -3.487998032647467, Steps: 200\n",
            "Episode: 927, Reward: -7.742443681056773, Steps: 200\n",
            "Episode: 928, Reward: -8.186059727654243, Steps: 200\n",
            "Episode: 929, Reward: -2.444322463120557, Steps: 200\n",
            "Episode: 930, Reward: -5.431909915083791, Steps: 200\n",
            "Episode: 931, Reward: -5.35838268807436, Steps: 200\n",
            "Episode: 932, Reward: -3.5426894037786023, Steps: 200\n",
            "Episode: 933, Reward: -2.591824261476686, Steps: 200\n",
            "Episode: 934, Reward: -2.679676781317076, Steps: 200\n",
            "Episode: 935, Reward: -9.928357805565788, Steps: 200\n",
            "Episode: 936, Reward: -1.5725836597397487, Steps: 200\n",
            "Episode: 937, Reward: -1.8425346572853603, Steps: 200\n",
            "Episode: 938, Reward: -19.66787793540357, Steps: 200\n",
            "Episode: 939, Reward: -9.918726193405423, Steps: 200\n",
            "Episode: 940, Reward: -3.0994812475025086, Steps: 200\n",
            "Episode: 941, Reward: -3.356055432921462, Steps: 200\n",
            "Episode: 942, Reward: -9.572585444623815, Steps: 200\n",
            "Episode: 943, Reward: -371.35489722530554, Steps: 200\n",
            "Episode: 944, Reward: -6.058786098894113, Steps: 200\n",
            "Episode: 945, Reward: -3.6573636206632836, Steps: 200\n",
            "Episode: 946, Reward: -3.6031996311428642, Steps: 200\n",
            "Episode: 947, Reward: -2.880218387902862, Steps: 200\n",
            "Episode: 948, Reward: -34.27776194738333, Steps: 200\n",
            "Episode: 949, Reward: -4.590638089607149, Steps: 200\n",
            "Episode: 950, Reward: -3.6344208894249035, Steps: 200\n",
            "Episode: 951, Reward: -4.818770105663599, Steps: 200\n",
            "Episode: 952, Reward: -4.240468156195428, Steps: 200\n",
            "Episode: 953, Reward: -11.517383592822615, Steps: 200\n",
            "Episode: 954, Reward: -4.316653591951655, Steps: 200\n",
            "Episode: 955, Reward: -6.4396804997212485, Steps: 200\n",
            "Episode: 956, Reward: -603.2476296670376, Steps: 200\n",
            "Episode: 957, Reward: -165.21751663756385, Steps: 200\n",
            "Episode: 958, Reward: -1.910687824678525, Steps: 200\n",
            "Episode: 959, Reward: -7.531614649515605, Steps: 200\n",
            "Episode: 960, Reward: -3.358721672281328, Steps: 200\n",
            "Episode: 961, Reward: -4.610622533050272, Steps: 200\n",
            "Episode: 962, Reward: -3.6812923867801675, Steps: 200\n",
            "Episode: 963, Reward: -3.57302474869971, Steps: 200\n",
            "Episode: 964, Reward: -4.66410958429324, Steps: 200\n",
            "Episode: 965, Reward: -6.208895392922858, Steps: 200\n",
            "Episode: 966, Reward: -9.024681821998902, Steps: 200\n",
            "Episode: 967, Reward: -8.441217077705398, Steps: 200\n",
            "Episode: 968, Reward: -16.950917234768884, Steps: 200\n",
            "Episode: 969, Reward: -3.8226332173751145, Steps: 200\n",
            "Episode: 970, Reward: -4.1091545308828765, Steps: 200\n",
            "Episode: 971, Reward: -4.677948895909893, Steps: 200\n",
            "Episode: 972, Reward: -15.502751785566748, Steps: 200\n",
            "Episode: 973, Reward: -3.2795152777556837, Steps: 200\n",
            "Episode: 974, Reward: -12.287319206552965, Steps: 200\n",
            "Episode: 975, Reward: -7.4782273547146305, Steps: 200\n",
            "Episode: 976, Reward: -15.037165153002459, Steps: 200\n",
            "Episode: 977, Reward: -8.54864450086436, Steps: 200\n",
            "Episode: 978, Reward: -2.3755063713610527, Steps: 200\n",
            "Episode: 979, Reward: -15.041456147158655, Steps: 200\n",
            "Episode: 980, Reward: -5.056316016734196, Steps: 200\n",
            "Episode: 981, Reward: -4.226625355107044, Steps: 200\n",
            "Episode: 982, Reward: -39.51143598080068, Steps: 200\n",
            "Episode: 983, Reward: -6.899411343178524, Steps: 200\n",
            "Episode: 984, Reward: -3.230355970937849, Steps: 200\n",
            "Episode: 985, Reward: -8.970414871219402, Steps: 200\n",
            "Episode: 986, Reward: -2.765072107575594, Steps: 200\n",
            "Episode: 987, Reward: -334.3646327485887, Steps: 200\n",
            "Episode: 988, Reward: -12.61420618728252, Steps: 200\n",
            "Episode: 989, Reward: -7.026088353685982, Steps: 200\n",
            "Episode: 990, Reward: -2.4238046952005945, Steps: 200\n",
            "Episode: 991, Reward: -5.166359339926912, Steps: 200\n",
            "Episode: 992, Reward: -6.367280730298023, Steps: 200\n",
            "Episode: 993, Reward: -12.877338887236304, Steps: 200\n",
            "Episode: 994, Reward: -7.368404303491878, Steps: 200\n",
            "Episode: 995, Reward: -3.9438015420019825, Steps: 200\n",
            "Episode: 996, Reward: -2.590407079486625, Steps: 200\n",
            "Episode: 997, Reward: -3.676070078185904, Steps: 200\n",
            "Episode: 998, Reward: -5.905521853862512, Steps: 200\n",
            "Episode: 999, Reward: -11.467838819755888, Steps: 200\n",
            "Episode: 1000, Reward: -7.975135026725474, Steps: 200\n",
            "Episode: 1001, Reward: -4.39092139039462, Steps: 200\n",
            "Episode: 1002, Reward: -8.478630909788551, Steps: 200\n",
            "Episode: 1003, Reward: -7.222098533225864, Steps: 200\n",
            "Episode: 1004, Reward: -6.793790527606837, Steps: 200\n",
            "Episode: 1005, Reward: -11.909428882021393, Steps: 200\n",
            "Episode: 1006, Reward: -2.2976736489329737, Steps: 200\n",
            "Episode: 1007, Reward: -2.204667697849251, Steps: 200\n",
            "Episode: 1008, Reward: -4.11216252357915, Steps: 200\n",
            "Episode: 1009, Reward: -3.8735099049001263, Steps: 200\n",
            "Episode: 1010, Reward: -10.572127155664518, Steps: 200\n",
            "Episode: 1011, Reward: -7.5409194503265065, Steps: 200\n",
            "Episode: 1012, Reward: -9.144571069168862, Steps: 200\n",
            "Episode: 1013, Reward: -3.376994287921087, Steps: 200\n",
            "Episode: 1014, Reward: -4.7747086569828205, Steps: 200\n",
            "Episode: 1015, Reward: -584.6207757512808, Steps: 200\n",
            "Episode: 1016, Reward: -3.899233450676767, Steps: 200\n",
            "Episode: 1017, Reward: -8.073997393165973, Steps: 200\n",
            "Episode: 1018, Reward: -12.23426685546776, Steps: 200\n",
            "Episode: 1019, Reward: -2.263387559198413, Steps: 200\n",
            "Episode: 1020, Reward: -5.649193652877824, Steps: 200\n",
            "Episode: 1021, Reward: -6.801936185443078, Steps: 200\n",
            "Episode: 1022, Reward: -3.5048770309514965, Steps: 200\n",
            "Episode: 1023, Reward: -2.0955632965623696, Steps: 200\n",
            "Episode: 1024, Reward: -6.65677433190533, Steps: 200\n",
            "Episode: 1025, Reward: -5.133956347819605, Steps: 200\n",
            "Episode: 1026, Reward: -5.426424955426104, Steps: 200\n",
            "Episode: 1027, Reward: -131.17731962426748, Steps: 200\n",
            "Episode: 1028, Reward: -4.625580528950413, Steps: 200\n",
            "Episode: 1029, Reward: -13.416290825733878, Steps: 200\n",
            "Episode: 1030, Reward: -5.2531511736718315, Steps: 200\n",
            "Episode: 1031, Reward: -3.0946898680270403, Steps: 200\n",
            "Episode: 1032, Reward: -8.374561012242365, Steps: 200\n",
            "Episode: 1033, Reward: -3.306607639537512, Steps: 200\n",
            "Episode: 1034, Reward: -3.112648388811652, Steps: 200\n",
            "Episode: 1035, Reward: -1.6324771935229345, Steps: 200\n",
            "Episode: 1036, Reward: -3.001682661953514, Steps: 200\n",
            "Episode: 1037, Reward: -7.165826042607746, Steps: 200\n",
            "Episode: 1038, Reward: -27.440401448668545, Steps: 200\n",
            "Episode: 1039, Reward: -3.8624996023444313, Steps: 200\n",
            "Episode: 1040, Reward: -11.799889443360259, Steps: 200\n",
            "Episode: 1041, Reward: -446.53713503901054, Steps: 200\n",
            "Episode: 1042, Reward: -7.89114478316301, Steps: 200\n",
            "Episode: 1043, Reward: -15.678426365926892, Steps: 200\n",
            "Episode: 1044, Reward: -25.420000038263858, Steps: 200\n",
            "Episode: 1045, Reward: -7.402669182584391, Steps: 200\n",
            "Episode: 1046, Reward: -6.205601763627753, Steps: 200\n",
            "Episode: 1047, Reward: -5.176692380017791, Steps: 200\n",
            "Episode: 1048, Reward: -31.11841477346284, Steps: 200\n",
            "Episode: 1049, Reward: -9.29747360031002, Steps: 200\n",
            "Episode: 1050, Reward: -44.40993509391437, Steps: 200\n",
            "Episode: 1051, Reward: -12.86503264698867, Steps: 200\n",
            "Episode: 1052, Reward: -3.6073937991340177, Steps: 200\n",
            "Episode: 1053, Reward: -9.690931545641408, Steps: 200\n",
            "Episode: 1054, Reward: -9.667120178392638, Steps: 200\n",
            "Episode: 1055, Reward: -1.8109810062329859, Steps: 200\n",
            "Episode: 1056, Reward: -5.160535223351914, Steps: 200\n",
            "Episode: 1057, Reward: -7.700158307000452, Steps: 200\n",
            "Episode: 1058, Reward: -8.123309826676334, Steps: 200\n",
            "Episode: 1059, Reward: -10.630603779708409, Steps: 200\n",
            "Episode: 1060, Reward: -7.608589037093063, Steps: 200\n",
            "Episode: 1061, Reward: -2.6783973566171313, Steps: 200\n",
            "Episode: 1062, Reward: -5.113913662742048, Steps: 200\n",
            "Episode: 1063, Reward: -3.9502577720626904, Steps: 200\n",
            "Episode: 1064, Reward: -9.281309845386549, Steps: 200\n",
            "Episode: 1065, Reward: -8.027190031435405, Steps: 200\n",
            "Episode: 1066, Reward: -5.646846967007021, Steps: 200\n",
            "Episode: 1067, Reward: -18.12383478082001, Steps: 200\n",
            "Episode: 1068, Reward: -6.070537584210814, Steps: 200\n",
            "Episode: 1069, Reward: -11.885911607149458, Steps: 200\n",
            "Episode: 1070, Reward: -6.033866633575379, Steps: 200\n",
            "Episode: 1071, Reward: -4.238893689425212, Steps: 200\n",
            "Episode: 1072, Reward: -6.787894307475036, Steps: 200\n",
            "Episode: 1073, Reward: -4.007295497028073, Steps: 200\n",
            "Episode: 1074, Reward: -3.2409917179834724, Steps: 200\n",
            "Episode: 1075, Reward: -3.6983390928889532, Steps: 200\n",
            "Episode: 1076, Reward: -5.9596916826240065, Steps: 200\n",
            "Episode: 1077, Reward: -2.2795496200734315, Steps: 200\n",
            "Episode: 1078, Reward: -5.012301558120792, Steps: 200\n",
            "Episode: 1079, Reward: -2.7703975785567976, Steps: 200\n",
            "Episode: 1080, Reward: -15.384291663654619, Steps: 200\n",
            "Episode: 1081, Reward: -5.031690997778088, Steps: 200\n",
            "Episode: 1082, Reward: -4.43882101825508, Steps: 200\n",
            "Episode: 1083, Reward: -8.439152515212683, Steps: 200\n",
            "Episode: 1084, Reward: -6.117264291966897, Steps: 200\n",
            "Episode: 1085, Reward: -5.630439339504227, Steps: 200\n",
            "Episode: 1086, Reward: -6.319280435174484, Steps: 200\n",
            "Episode: 1087, Reward: -4.913364594991778, Steps: 200\n",
            "Episode: 1088, Reward: -7.113803517684857, Steps: 200\n",
            "Episode: 1089, Reward: -1.3433153270502685, Steps: 200\n",
            "Episode: 1090, Reward: -2.4206557062653307, Steps: 200\n",
            "Episode: 1091, Reward: -5.684836073240534, Steps: 200\n",
            "Episode: 1092, Reward: -2.871374984659884, Steps: 200\n",
            "Episode: 1093, Reward: -5.7599513972049285, Steps: 200\n",
            "Episode: 1094, Reward: -8.605692753864057, Steps: 200\n",
            "Episode: 1095, Reward: -2.2635232207957747, Steps: 200\n",
            "Episode: 1096, Reward: -2.426432355756602, Steps: 200\n",
            "Episode: 1097, Reward: -2.4513748821365176, Steps: 200\n",
            "Episode: 1098, Reward: -4.331054085065313, Steps: 200\n",
            "Episode: 1099, Reward: -2.1402916033358554, Steps: 200\n",
            "Episode: 1100, Reward: -7.06697248843734, Steps: 200\n",
            "Episode: 1101, Reward: -8.022371112527168, Steps: 200\n",
            "Episode: 1102, Reward: -1.8222450315789274, Steps: 200\n",
            "Episode: 1103, Reward: -4.114101751108611, Steps: 200\n",
            "Episode: 1104, Reward: -7.221701978019524, Steps: 200\n",
            "Episode: 1105, Reward: -4.721574859177121, Steps: 200\n",
            "Episode: 1106, Reward: -2.880488847172305, Steps: 200\n",
            "Episode: 1107, Reward: -7.0024405678944595, Steps: 200\n",
            "Episode: 1108, Reward: -2.0610612861497186, Steps: 200\n",
            "Episode: 1109, Reward: -2.681183202203667, Steps: 200\n",
            "Episode: 1110, Reward: -2.6328952350310892, Steps: 200\n",
            "Episode: 1111, Reward: -17.330532327830248, Steps: 200\n",
            "Episode: 1112, Reward: -3.860187198103892, Steps: 200\n",
            "Episode: 1113, Reward: -9.84626867319428, Steps: 200\n",
            "Episode: 1114, Reward: -3.454859622411334, Steps: 200\n",
            "Episode: 1115, Reward: -2.2246711771451793, Steps: 200\n",
            "Episode: 1116, Reward: -3.574002870690809, Steps: 200\n",
            "Episode: 1117, Reward: -2.2865152649499985, Steps: 200\n",
            "Episode: 1118, Reward: -2.807470976417117, Steps: 200\n",
            "Episode: 1119, Reward: -5.1548259482445635, Steps: 200\n",
            "Episode: 1120, Reward: -1.984405917006072, Steps: 200\n",
            "Episode: 1121, Reward: -6.273633947073279, Steps: 200\n",
            "Episode: 1122, Reward: -1.9073897531928645, Steps: 200\n",
            "Episode: 1123, Reward: -10.593808472521092, Steps: 200\n",
            "Episode: 1124, Reward: -8.907355835962194, Steps: 200\n",
            "Episode: 1125, Reward: -2.8801991016344126, Steps: 200\n",
            "Episode: 1126, Reward: -1278.3201961003558, Steps: 200\n",
            "Episode: 1127, Reward: -18.805863337224153, Steps: 200\n",
            "Episode: 1128, Reward: -7.658066347190925, Steps: 200\n",
            "Episode: 1129, Reward: -5.342066144179605, Steps: 200\n",
            "Episode: 1130, Reward: -3.6225763701679496, Steps: 200\n",
            "Episode: 1131, Reward: -4.531560587015193, Steps: 200\n",
            "Episode: 1132, Reward: -1.9004042050297552, Steps: 200\n",
            "Episode: 1133, Reward: -7.234093205474616, Steps: 200\n",
            "Episode: 1134, Reward: -4.683918789077713, Steps: 200\n",
            "Episode: 1135, Reward: -380.81513263021867, Steps: 200\n",
            "Episode: 1136, Reward: -2.2317723231110818, Steps: 200\n",
            "Episode: 1137, Reward: -4.913430990729167, Steps: 200\n",
            "Episode: 1138, Reward: -13.724580994581055, Steps: 200\n",
            "Episode: 1139, Reward: -4.510343282056955, Steps: 200\n",
            "Episode: 1140, Reward: -4.922371461080815, Steps: 200\n",
            "Episode: 1141, Reward: -7.188564617722915, Steps: 200\n",
            "Episode: 1142, Reward: -3.143546613902467, Steps: 200\n",
            "Episode: 1143, Reward: -2.3058811400823878, Steps: 200\n",
            "Episode: 1144, Reward: -4.0199069309632645, Steps: 200\n",
            "Episode: 1145, Reward: -2.934986100077231, Steps: 200\n",
            "Episode: 1146, Reward: -6.49228087418118, Steps: 200\n",
            "Episode: 1147, Reward: -3.816097314551729, Steps: 200\n",
            "Episode: 1148, Reward: -3.092178364510502, Steps: 200\n",
            "Episode: 1149, Reward: -2.7483178550087817, Steps: 200\n",
            "Episode: 1150, Reward: -2.7215813829600615, Steps: 200\n",
            "Episode: 1151, Reward: -3.3622935831178618, Steps: 200\n",
            "Episode: 1152, Reward: -8.12358424451246, Steps: 200\n",
            "Episode: 1153, Reward: -11.619848132788203, Steps: 200\n",
            "Episode: 1154, Reward: -5.030015166409877, Steps: 200\n",
            "Episode: 1155, Reward: -4.725485328100073, Steps: 200\n",
            "Episode: 1156, Reward: -2.8866946725066276, Steps: 200\n",
            "Episode: 1157, Reward: -12.90669849861897, Steps: 200\n",
            "Episode: 1158, Reward: -2.681170816881437, Steps: 200\n",
            "Episode: 1159, Reward: -2.4886560907345956, Steps: 200\n",
            "Episode: 1160, Reward: -4.26385930684262, Steps: 200\n",
            "Episode: 1161, Reward: -3.6044707656316706, Steps: 200\n",
            "Episode: 1162, Reward: -4.584746627452207, Steps: 200\n",
            "Episode: 1163, Reward: -1.348703495457932, Steps: 200\n",
            "Episode: 1164, Reward: -2.0993024878610678, Steps: 200\n",
            "Episode: 1165, Reward: -2.989119267654433, Steps: 200\n",
            "Episode: 1166, Reward: -1.2749282078224984, Steps: 200\n",
            "Episode: 1167, Reward: -1.9097419562263795, Steps: 200\n",
            "Episode: 1168, Reward: -3.695404714790255, Steps: 200\n",
            "Episode: 1169, Reward: -0.8942531439197782, Steps: 200\n",
            "Episode: 1170, Reward: -2.8153775368573335, Steps: 200\n",
            "Episode: 1171, Reward: -17.487907325705233, Steps: 200\n",
            "Episode: 1172, Reward: -3.8903626505210984, Steps: 200\n",
            "Episode: 1173, Reward: -2.1577733032984345, Steps: 200\n",
            "Episode: 1174, Reward: -3.5803915059526625, Steps: 200\n",
            "Episode: 1175, Reward: -6.988837509121778, Steps: 200\n",
            "Episode: 1176, Reward: -3.1741836797404797, Steps: 200\n",
            "Episode: 1177, Reward: -2.9030092779765733, Steps: 200\n",
            "Episode: 1178, Reward: -2.93831283301925, Steps: 200\n",
            "Episode: 1179, Reward: -1.5956751204422028, Steps: 200\n",
            "Episode: 1180, Reward: -1.4529821144618034, Steps: 200\n",
            "Episode: 1181, Reward: -441.48095345492214, Steps: 200\n",
            "Episode: 1182, Reward: -5.473593145255631, Steps: 200\n",
            "Episode: 1183, Reward: -10.57544614975247, Steps: 200\n",
            "Episode: 1184, Reward: -253.03474264914, Steps: 200\n",
            "Episode: 1185, Reward: -2.1174890481424646, Steps: 200\n",
            "Episode: 1186, Reward: -7.173022463151309, Steps: 200\n",
            "Episode: 1187, Reward: -2.9050147279621488, Steps: 200\n",
            "Episode: 1188, Reward: -2.293885322561534, Steps: 200\n",
            "Episode: 1189, Reward: -5.815229395182448, Steps: 200\n",
            "Episode: 1190, Reward: -2.986068471087438, Steps: 200\n",
            "Episode: 1191, Reward: -4.9615846612425605, Steps: 200\n",
            "Episode: 1192, Reward: -2.6268899710102773, Steps: 200\n",
            "Episode: 1193, Reward: -4.121159998912618, Steps: 200\n",
            "Episode: 1194, Reward: -1.8560352443841084, Steps: 200\n",
            "Episode: 1195, Reward: -5.74674891908041, Steps: 200\n",
            "Episode: 1196, Reward: -5.152955412424192, Steps: 200\n",
            "Episode: 1197, Reward: -5.176342508511006, Steps: 200\n",
            "Episode: 1198, Reward: -2.9715286027120733, Steps: 200\n",
            "Episode: 1199, Reward: -4.567978092550115, Steps: 200\n",
            "Episode: 1200, Reward: -1.8022103290824933, Steps: 200\n",
            "Episode: 1201, Reward: -48.836057058392875, Steps: 200\n",
            "Episode: 1202, Reward: -1.7038151450359067, Steps: 200\n",
            "Episode: 1203, Reward: -2.549663226264559, Steps: 200\n",
            "Episode: 1204, Reward: -284.46391925646304, Steps: 200\n",
            "Episode: 1205, Reward: -3.1390516596029356, Steps: 200\n",
            "Episode: 1206, Reward: -9.957404117478388, Steps: 200\n",
            "Episode: 1207, Reward: -2.6205001261835577, Steps: 200\n",
            "Episode: 1208, Reward: -1.8628326918649767, Steps: 200\n",
            "Episode: 1209, Reward: -9.31550166766706, Steps: 200\n",
            "Episode: 1210, Reward: -297.63409117240474, Steps: 200\n",
            "Episode: 1211, Reward: -1.38426340630817, Steps: 200\n",
            "Episode: 1212, Reward: -5.528512172300949, Steps: 200\n",
            "Episode: 1213, Reward: -1.8896964809694463, Steps: 200\n",
            "Episode: 1214, Reward: -4.18068353380815, Steps: 200\n",
            "Episode: 1215, Reward: -3.881977502069856, Steps: 200\n",
            "Episode: 1216, Reward: -10.117607568349893, Steps: 200\n",
            "Episode: 1217, Reward: -4.740407381049636, Steps: 200\n",
            "Episode: 1218, Reward: -13.519329976246764, Steps: 200\n",
            "Episode: 1219, Reward: -5.501097219935174, Steps: 200\n",
            "Episode: 1220, Reward: -4.164792091342726, Steps: 200\n",
            "Episode: 1221, Reward: -8.871885098050855, Steps: 200\n",
            "Episode: 1222, Reward: -7.4186258862041905, Steps: 200\n",
            "Episode: 1223, Reward: -6.256330654113994, Steps: 200\n",
            "Episode: 1224, Reward: -4.140642888755918, Steps: 200\n",
            "Episode: 1225, Reward: -12.676213509835204, Steps: 200\n",
            "Episode: 1226, Reward: -2.668191381601932, Steps: 200\n",
            "Episode: 1227, Reward: -3.9761972137846735, Steps: 200\n",
            "Episode: 1228, Reward: -8.452102618536497, Steps: 200\n",
            "Episode: 1229, Reward: -4.974693794919017, Steps: 200\n",
            "Episode: 1230, Reward: -2.029455088168337, Steps: 200\n",
            "Episode: 1231, Reward: -1.9943333585178225, Steps: 200\n",
            "Episode: 1232, Reward: -1.9050198460623013, Steps: 200\n",
            "Episode: 1233, Reward: -4.843255651838212, Steps: 200\n",
            "Episode: 1234, Reward: -1.7717694856212554, Steps: 200\n",
            "Episode: 1235, Reward: -6.155200731178944, Steps: 200\n",
            "Episode: 1236, Reward: -8.839044716410838, Steps: 200\n",
            "Episode: 1237, Reward: -2.423851806402703, Steps: 200\n",
            "Episode: 1238, Reward: -0.904596552250875, Steps: 200\n",
            "Episode: 1239, Reward: -38.98684945642165, Steps: 200\n",
            "Episode: 1240, Reward: -6.520826731339663, Steps: 200\n",
            "Episode: 1241, Reward: -3.772041709841494, Steps: 200\n",
            "Episode: 1242, Reward: -11.170884328624048, Steps: 200\n",
            "Episode: 1243, Reward: -4.23696319908238, Steps: 200\n",
            "Episode: 1244, Reward: -4.8258203144975855, Steps: 200\n",
            "Episode: 1245, Reward: -12.61624067992354, Steps: 200\n",
            "Episode: 1246, Reward: -13.92726748665292, Steps: 200\n",
            "Episode: 1247, Reward: -9.516498392916665, Steps: 200\n",
            "Episode: 1248, Reward: -1.2977304386139326, Steps: 200\n",
            "Episode: 1249, Reward: -2.438828997309484, Steps: 200\n",
            "Episode: 1250, Reward: -7.099995232129104, Steps: 200\n",
            "Episode: 1251, Reward: -1.84067799719566, Steps: 200\n",
            "Episode: 1252, Reward: -4.466090430679272, Steps: 200\n",
            "Episode: 1253, Reward: -3.680845607061748, Steps: 200\n",
            "Episode: 1254, Reward: -3.0602263120774915, Steps: 200\n",
            "Episode: 1255, Reward: -2.1251248678508055, Steps: 200\n",
            "Episode: 1256, Reward: -3.0545907714906066, Steps: 200\n",
            "Episode: 1257, Reward: -2.7879037439219205, Steps: 200\n",
            "Episode: 1258, Reward: -5.553185185452052, Steps: 200\n",
            "Episode: 1259, Reward: -5.33392573957741, Steps: 200\n",
            "Episode: 1260, Reward: -9.886999980438842, Steps: 200\n",
            "Episode: 1261, Reward: -1.5160910783699257, Steps: 200\n",
            "Episode: 1262, Reward: -1.1963179475220012, Steps: 200\n",
            "Episode: 1263, Reward: -3.316914853931961, Steps: 200\n",
            "Episode: 1264, Reward: -7.974937782871791, Steps: 200\n",
            "Episode: 1265, Reward: -3.6957167029679576, Steps: 200\n",
            "Episode: 1266, Reward: -6.766191387071099, Steps: 200\n",
            "Episode: 1267, Reward: -3.170021292092898, Steps: 200\n",
            "Episode: 1268, Reward: -6.084351398800436, Steps: 200\n",
            "Episode: 1269, Reward: -6.421394844073753, Steps: 200\n",
            "Episode: 1270, Reward: -6.567169154804592, Steps: 200\n",
            "Episode: 1271, Reward: -10.597650781413632, Steps: 200\n",
            "Episode: 1272, Reward: -4.705852127133151, Steps: 200\n",
            "Episode: 1273, Reward: -2.0382282893173302, Steps: 200\n",
            "Episode: 1274, Reward: -2.7768656483044736, Steps: 200\n",
            "Episode: 1275, Reward: -1.47013983556308, Steps: 200\n",
            "Episode: 1276, Reward: -1.8245104698334047, Steps: 200\n",
            "Episode: 1277, Reward: -3.4523429412637494, Steps: 200\n",
            "Episode: 1278, Reward: -9.176102740940898, Steps: 200\n",
            "Episode: 1279, Reward: -1.7072659158702412, Steps: 200\n",
            "Episode: 1280, Reward: -3.1440187400182897, Steps: 200\n",
            "Episode: 1281, Reward: -2.700287180317286, Steps: 200\n",
            "Episode: 1282, Reward: -4.527977474623886, Steps: 200\n",
            "Episode: 1283, Reward: -2.0712798177228193, Steps: 200\n",
            "Episode: 1284, Reward: -6.045583179376132, Steps: 200\n",
            "Episode: 1285, Reward: -2.0461167190147638, Steps: 200\n",
            "Episode: 1286, Reward: -4.332185777789288, Steps: 200\n",
            "Episode: 1287, Reward: -6.585364616587709, Steps: 200\n",
            "Episode: 1288, Reward: -2.90524656914081, Steps: 200\n",
            "Episode: 1289, Reward: -4.019425436243492, Steps: 200\n",
            "Episode: 1290, Reward: -2.046347908541625, Steps: 200\n",
            "Episode: 1291, Reward: -9.316015703156186, Steps: 200\n",
            "Episode: 1292, Reward: -6.1866512742898685, Steps: 200\n",
            "Episode: 1293, Reward: -2.618287440625161, Steps: 200\n",
            "Episode: 1294, Reward: -2.9733715913188274, Steps: 200\n",
            "Episode: 1295, Reward: -5.561113658137335, Steps: 200\n",
            "Episode: 1296, Reward: -0.7642600332226048, Steps: 200\n",
            "Episode: 1297, Reward: -25.238496706699372, Steps: 200\n",
            "Episode: 1298, Reward: -70.85147138880374, Steps: 200\n",
            "Episode: 1299, Reward: -661.7454089124333, Steps: 200\n",
            "Episode: 1300, Reward: -3.0123082015934464, Steps: 200\n",
            "Episode: 1301, Reward: -1.6859222309971325, Steps: 200\n",
            "Episode: 1302, Reward: -534.7232544657584, Steps: 200\n",
            "Episode: 1303, Reward: -2.84672185574106, Steps: 200\n",
            "Episode: 1304, Reward: -472.0131164752667, Steps: 200\n",
            "Episode: 1305, Reward: -413.72143613739763, Steps: 200\n",
            "Episode: 1306, Reward: -750.1767740772368, Steps: 200\n",
            "Episode: 1307, Reward: -966.7592098171776, Steps: 200\n",
            "Episode: 1308, Reward: -5.500531240601737, Steps: 200\n",
            "Episode: 1309, Reward: -2.4833196868930556, Steps: 200\n",
            "Episode: 1310, Reward: -2.3932679306115885, Steps: 200\n",
            "Episode: 1311, Reward: -3.362927787428877, Steps: 200\n",
            "Episode: 1312, Reward: -2.7522415892867858, Steps: 200\n",
            "Episode: 1313, Reward: -3.3905870723587004, Steps: 200\n",
            "Episode: 1314, Reward: -10.78841030951112, Steps: 200\n",
            "Episode: 1315, Reward: -6.99157411718388, Steps: 200\n",
            "Episode: 1316, Reward: -1.743270824847054, Steps: 200\n",
            "Episode: 1317, Reward: -3.5776268119485546, Steps: 200\n",
            "Episode: 1318, Reward: -3.862671493976788, Steps: 200\n",
            "Episode: 1319, Reward: -2.5530495707636884, Steps: 200\n",
            "Episode: 1320, Reward: -6.5496134491640845, Steps: 200\n",
            "Episode: 1321, Reward: -3.723662390375613, Steps: 200\n",
            "Episode: 1322, Reward: -2.414517374306325, Steps: 200\n",
            "Episode: 1323, Reward: -7.484943864522817, Steps: 200\n",
            "Episode: 1324, Reward: -3.335755623821426, Steps: 200\n",
            "Episode: 1325, Reward: -534.2930987237339, Steps: 200\n",
            "Episode: 1326, Reward: -5.877722583309779, Steps: 200\n",
            "Episode: 1327, Reward: -9.153510858371773, Steps: 200\n",
            "Episode: 1328, Reward: -2.2071171532082086, Steps: 200\n",
            "Episode: 1329, Reward: -2.413948762469196, Steps: 200\n",
            "Episode: 1330, Reward: -1.7469516869159247, Steps: 200\n",
            "Episode: 1331, Reward: -4.593062231672706, Steps: 200\n",
            "Episode: 1332, Reward: -18.159116314574955, Steps: 200\n",
            "Episode: 1333, Reward: -2.0054695346583937, Steps: 200\n",
            "Episode: 1334, Reward: -2.8702185174945893, Steps: 200\n",
            "Episode: 1335, Reward: -3.3909434869740207, Steps: 200\n",
            "Episode: 1336, Reward: -1.8242468076915561, Steps: 200\n",
            "Episode: 1337, Reward: -8.140649340940561, Steps: 200\n",
            "Episode: 1338, Reward: -9.613612153845208, Steps: 200\n",
            "Episode: 1339, Reward: -4.248931307129157, Steps: 200\n",
            "Episode: 1340, Reward: -3.0984767824981296, Steps: 200\n",
            "Episode: 1341, Reward: -7.281811127031049, Steps: 200\n",
            "Episode: 1342, Reward: -388.71106150869707, Steps: 200\n",
            "Episode: 1343, Reward: -5.907947314653611, Steps: 200\n",
            "Episode: 1344, Reward: -230.81118760416695, Steps: 200\n",
            "Episode: 1345, Reward: -3.3419709056038727, Steps: 200\n",
            "Episode: 1346, Reward: -286.656706356318, Steps: 200\n",
            "Episode: 1347, Reward: -4.872136959565661, Steps: 200\n",
            "Episode: 1348, Reward: -3.2381317307540063, Steps: 200\n",
            "Episode: 1349, Reward: -5.005896262243721, Steps: 200\n",
            "Episode: 1350, Reward: -3.4878717470689833, Steps: 200\n",
            "Episode: 1351, Reward: -4.595611261787047, Steps: 200\n",
            "Episode: 1352, Reward: -0.8673662203791633, Steps: 200\n",
            "Episode: 1353, Reward: -26.945597982398574, Steps: 200\n",
            "Episode: 1354, Reward: -11.45422650575025, Steps: 200\n",
            "Episode: 1355, Reward: -12.881777987967332, Steps: 200\n",
            "Episode: 1356, Reward: -413.46028897883036, Steps: 200\n",
            "Episode: 1357, Reward: -2.8166991464004294, Steps: 200\n",
            "Episode: 1358, Reward: -291.4214991134215, Steps: 200\n",
            "Episode: 1359, Reward: -802.1995034488598, Steps: 200\n",
            "Episode: 1360, Reward: -400.3807585931417, Steps: 200\n",
            "Episode: 1361, Reward: -0.8788289416864085, Steps: 200\n",
            "Episode: 1362, Reward: -3.7833010730326184, Steps: 200\n",
            "Episode: 1363, Reward: -791.1209538910871, Steps: 200\n",
            "Episode: 1364, Reward: -8.275209504903884, Steps: 200\n",
            "Episode: 1365, Reward: -16.750608816831836, Steps: 200\n",
            "Episode: 1366, Reward: -5.337897818614269, Steps: 200\n",
            "Episode: 1367, Reward: -5.957732285979139, Steps: 200\n",
            "Episode: 1368, Reward: -5.35041320397495, Steps: 200\n",
            "Episode: 1369, Reward: -18.82041751518108, Steps: 200\n",
            "Episode: 1370, Reward: -4.607110244057939, Steps: 200\n",
            "Episode: 1371, Reward: -11.280444748348037, Steps: 200\n",
            "Episode: 1372, Reward: -4.109732821460167, Steps: 200\n",
            "Episode: 1373, Reward: -3.5800160512167305, Steps: 200\n",
            "Episode: 1374, Reward: -3.1459151624732744, Steps: 200\n",
            "Episode: 1375, Reward: -8.22258193093761, Steps: 200\n",
            "Episode: 1376, Reward: -8.431678288470803, Steps: 200\n",
            "Episode: 1377, Reward: -6.196002321114796, Steps: 200\n",
            "Episode: 1378, Reward: -7.637549335162139, Steps: 200\n",
            "Episode: 1379, Reward: -2.373535426016576, Steps: 200\n",
            "Episode: 1380, Reward: -5.77215264050618, Steps: 200\n",
            "Episode: 1381, Reward: -13.095055849423881, Steps: 200\n",
            "Episode: 1382, Reward: -2.1337271130909903, Steps: 200\n",
            "Episode: 1383, Reward: -10.90684685925231, Steps: 200\n",
            "Episode: 1384, Reward: -1.2595228785531394, Steps: 200\n",
            "Episode: 1385, Reward: -2.647070065721537, Steps: 200\n",
            "Episode: 1386, Reward: -10.247851456680717, Steps: 200\n",
            "Episode: 1387, Reward: -5.506273014639053, Steps: 200\n",
            "Episode: 1388, Reward: -1.672989063884221, Steps: 200\n",
            "Episode: 1389, Reward: -1.3498355215563864, Steps: 200\n",
            "Episode: 1390, Reward: -1.1986903513937859, Steps: 200\n",
            "Episode: 1391, Reward: -5.036488652030502, Steps: 200\n",
            "Episode: 1392, Reward: -10.165704582552438, Steps: 200\n",
            "Episode: 1393, Reward: -147.73455236984512, Steps: 200\n",
            "Episode: 1394, Reward: -7.206186941114632, Steps: 200\n",
            "Episode: 1395, Reward: -6.669415935289548, Steps: 200\n",
            "Episode: 1396, Reward: -1.9695235695234738, Steps: 200\n",
            "Episode: 1397, Reward: -1.4837793896697524, Steps: 200\n",
            "Episode: 1398, Reward: -4.42534393926381, Steps: 200\n",
            "Episode: 1399, Reward: -5.104900537566544, Steps: 200\n",
            "Episode: 1400, Reward: -3.97471109323233, Steps: 200\n",
            "Episode: 1401, Reward: -9.694183696368594, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-04/q_network_ep_1401.pth\n",
            "\n",
            "Episode: 1402, Reward: -3.219659999115903, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-05/q_network_ep_1402.pth\n",
            "\n",
            "Episode: 1403, Reward: -2.380075469317564, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-06/q_network_ep_1403.pth\n",
            "\n",
            "Episode: 1404, Reward: -2.7169133514523174, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-07/q_network_ep_1404.pth\n",
            "\n",
            "Episode: 1405, Reward: -3.9310035816702023, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-08/q_network_ep_1405.pth\n",
            "\n",
            "Episode: 1406, Reward: -1.182081186984029, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-09/q_network_ep_1406.pth\n",
            "\n",
            "Episode: 1407, Reward: -6.1523354843792, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-10/q_network_ep_1407.pth\n",
            "\n",
            "Episode: 1408, Reward: -9.143217547681616, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-11/q_network_ep_1408.pth\n",
            "\n",
            "Episode: 1409, Reward: -6.452178837755666, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-12/q_network_ep_1409.pth\n",
            "\n",
            "Episode: 1410, Reward: -2.4223110691288796, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-12/q_network_ep_1410.pth\n",
            "\n",
            "Episode: 1411, Reward: -3.978704096446206, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-13/q_network_ep_1411.pth\n",
            "\n",
            "Episode: 1412, Reward: -3.7829815088082936, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-14/q_network_ep_1412.pth\n",
            "\n",
            "Episode: 1413, Reward: -4.395098989031742, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-15/q_network_ep_1413.pth\n",
            "\n",
            "Episode: 1414, Reward: -9.920028268774404, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-16/q_network_ep_1414.pth\n",
            "\n",
            "Episode: 1415, Reward: -0.9825707038194573, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-17/q_network_ep_1415.pth\n",
            "\n",
            "Episode: 1416, Reward: -1.8069979298998011, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-17/q_network_ep_1416.pth\n",
            "\n",
            "Episode: 1417, Reward: -2.469180197949857, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-19/q_network_ep_1417.pth\n",
            "\n",
            "Episode: 1418, Reward: -1.6882543692472152, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-20/q_network_ep_1418.pth\n",
            "\n",
            "Episode: 1419, Reward: -5.882197826187177, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-21/q_network_ep_1419.pth\n",
            "\n",
            "Episode: 1420, Reward: -3.093848368800959, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-22/q_network_ep_1420.pth\n",
            "\n",
            "Episode: 1421, Reward: -4.663589252956482, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-23/q_network_ep_1421.pth\n",
            "\n",
            "Episode: 1422, Reward: -2.0234615081369074, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-23/q_network_ep_1422.pth\n",
            "\n",
            "Episode: 1423, Reward: -4.812844254965029, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-24/q_network_ep_1423.pth\n",
            "\n",
            "Episode: 1424, Reward: -1.4975901221797292, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-25/q_network_ep_1424.pth\n",
            "\n",
            "Episode: 1425, Reward: -4.789304872023183, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-26/q_network_ep_1425.pth\n",
            "\n",
            "Episode: 1426, Reward: -2.863484169925164, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-27/q_network_ep_1426.pth\n",
            "\n",
            "Episode: 1427, Reward: -1.8183547267968627, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-28/q_network_ep_1427.pth\n",
            "\n",
            "Episode: 1428, Reward: -20.309303467737774, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-29/q_network_ep_1428.pth\n",
            "\n",
            "Episode: 1429, Reward: -7.089904458992612, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-30/q_network_ep_1429.pth\n",
            "\n",
            "Episode: 1430, Reward: -3.870190599752241, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-30/q_network_ep_1430.pth\n",
            "\n",
            "Episode: 1431, Reward: -1.7599249048467904, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-32/q_network_ep_1431.pth\n",
            "\n",
            "Episode: 1432, Reward: -1.6016490819209828, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-33/q_network_ep_1432.pth\n",
            "\n",
            "Episode: 1433, Reward: -3.3963251699355217, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-34/q_network_ep_1433.pth\n",
            "\n",
            "Episode: 1434, Reward: -737.6537044082448, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-35/q_network_ep_1434.pth\n",
            "\n",
            "Episode: 1435, Reward: -2.2116197009285874, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-35/q_network_ep_1435.pth\n",
            "\n",
            "Episode: 1436, Reward: -4.202005370725844, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-36/q_network_ep_1436.pth\n",
            "\n",
            "Episode: 1437, Reward: -3.0585370945437513, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-37/q_network_ep_1437.pth\n",
            "\n",
            "Episode: 1438, Reward: -1.644765267782873, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-38/q_network_ep_1438.pth\n",
            "\n",
            "Episode: 1439, Reward: -2.6418646450309384, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-39/q_network_ep_1439.pth\n",
            "\n",
            "Episode: 1440, Reward: -2.106196534003827, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-40/q_network_ep_1440.pth\n",
            "\n",
            "Episode: 1441, Reward: -0.8347726376652042, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-40/q_network_ep_1441.pth\n",
            "\n",
            "Episode: 1442, Reward: -5.354593465057488, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-41/q_network_ep_1442.pth\n",
            "\n",
            "Episode: 1443, Reward: -7.358598078494893, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-42/q_network_ep_1443.pth\n",
            "\n",
            "Episode: 1444, Reward: -435.03021695152626, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-43/q_network_ep_1444.pth\n",
            "\n",
            "Episode: 1445, Reward: -3.310170674308333, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-44/q_network_ep_1445.pth\n",
            "\n",
            "Episode: 1446, Reward: -5.479222456412228, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-45/q_network_ep_1446.pth\n",
            "\n",
            "Episode: 1447, Reward: -13.737553533646514, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-46/q_network_ep_1447.pth\n",
            "\n",
            "Episode: 1448, Reward: -2.6529055508626738, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-47/q_network_ep_1448.pth\n",
            "\n",
            "Episode: 1449, Reward: -5.678824778241925, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-48/q_network_ep_1449.pth\n",
            "\n",
            "Episode: 1450, Reward: -2.3566829429319145, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-49/q_network_ep_1450.pth\n",
            "\n",
            "Episode: 1451, Reward: -139.7138206526393, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-50/q_network_ep_1451.pth\n",
            "\n",
            "Episode: 1452, Reward: -2.2945949082066623, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-50/q_network_ep_1452.pth\n",
            "\n",
            "Episode: 1453, Reward: -6.324239306722458, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-51/q_network_ep_1453.pth\n",
            "\n",
            "Episode: 1454, Reward: -53.85177245078229, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-52/q_network_ep_1454.pth\n",
            "\n",
            "Episode: 1455, Reward: -2.751880114225926, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-53/q_network_ep_1455.pth\n",
            "\n",
            "Episode: 1456, Reward: -13.349868919949774, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-54/q_network_ep_1456.pth\n",
            "\n",
            "Episode: 1457, Reward: -2.4456199922019946, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-55/q_network_ep_1457.pth\n",
            "\n",
            "Episode: 1458, Reward: -3.9254742545353323, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-56/q_network_ep_1458.pth\n",
            "\n",
            "Episode: 1459, Reward: -3.918293302098311, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-58/q_network_ep_1459.pth\n",
            "\n",
            "Episode: 1460, Reward: -3.8981870327933215, Steps: 200\n",
            "model saved to models/2023-05-02_15-42-59/q_network_ep_1460.pth\n",
            "\n",
            "Episode: 1461, Reward: -2.719366160166033, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-00/q_network_ep_1461.pth\n",
            "\n",
            "Episode: 1462, Reward: -13.802855784903727, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-01/q_network_ep_1462.pth\n",
            "\n",
            "Episode: 1463, Reward: -5.297422543450167, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-01/q_network_ep_1463.pth\n",
            "\n",
            "Episode: 1464, Reward: -1.551127385786613, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-02/q_network_ep_1464.pth\n",
            "\n",
            "Episode: 1465, Reward: -79.17753520204029, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-03/q_network_ep_1465.pth\n",
            "\n",
            "Episode: 1466, Reward: -4.0311463850944795, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-04/q_network_ep_1466.pth\n",
            "\n",
            "Episode: 1467, Reward: -5.841201377692691, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-05/q_network_ep_1467.pth\n",
            "\n",
            "Episode: 1468, Reward: -3.913266797631005, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-06/q_network_ep_1468.pth\n",
            "\n",
            "Episode: 1469, Reward: -2.2370909763066518, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-06/q_network_ep_1469.pth\n",
            "\n",
            "Episode: 1470, Reward: -5.27770826199046, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-07/q_network_ep_1470.pth\n",
            "\n",
            "Episode: 1471, Reward: -1.7235053255439086, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-08/q_network_ep_1471.pth\n",
            "\n",
            "Episode: 1472, Reward: -3.5880043192369677, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-09/q_network_ep_1472.pth\n",
            "\n",
            "Episode: 1473, Reward: -3.5095778014487227, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-10/q_network_ep_1473.pth\n",
            "\n",
            "Episode: 1474, Reward: -158.66061295664687, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-11/q_network_ep_1474.pth\n",
            "\n",
            "Episode: 1475, Reward: -1.2224775384747282, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-12/q_network_ep_1475.pth\n",
            "\n",
            "Episode: 1476, Reward: -2.0681687984510213, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-13/q_network_ep_1476.pth\n",
            "\n",
            "Episode: 1477, Reward: -1.376848615461148, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-14/q_network_ep_1477.pth\n",
            "\n",
            "Episode: 1478, Reward: -7.151974323342391, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-15/q_network_ep_1478.pth\n",
            "\n",
            "Episode: 1479, Reward: -4.0753396091296015, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-16/q_network_ep_1479.pth\n",
            "\n",
            "Episode: 1480, Reward: -2.0273268603536168, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-17/q_network_ep_1480.pth\n",
            "\n",
            "Episode: 1481, Reward: -2.4264175641130135, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-17/q_network_ep_1481.pth\n",
            "\n",
            "Episode: 1482, Reward: -3.1342233590591264, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-18/q_network_ep_1482.pth\n",
            "\n",
            "Episode: 1483, Reward: -20.26013402929118, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-19/q_network_ep_1483.pth\n",
            "\n",
            "Episode: 1484, Reward: -82.8695821768005, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-20/q_network_ep_1484.pth\n",
            "\n",
            "Episode: 1485, Reward: -10.708659222752681, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-21/q_network_ep_1485.pth\n",
            "\n",
            "Episode: 1486, Reward: -1.571199645218068, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-21/q_network_ep_1486.pth\n",
            "\n",
            "Episode: 1487, Reward: -11.503897041870983, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-23/q_network_ep_1487.pth\n",
            "\n",
            "Episode: 1488, Reward: -5.347137160501791, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-24/q_network_ep_1488.pth\n",
            "\n",
            "Episode: 1489, Reward: -3.458817206405221, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-25/q_network_ep_1489.pth\n",
            "\n",
            "Episode: 1490, Reward: -1.0807674790879116, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-26/q_network_ep_1490.pth\n",
            "\n",
            "Episode: 1491, Reward: -4.845412568024341, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-27/q_network_ep_1491.pth\n",
            "\n",
            "Episode: 1492, Reward: -30.060491417611445, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-28/q_network_ep_1492.pth\n",
            "\n",
            "Episode: 1493, Reward: -2.6912210958484, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-29/q_network_ep_1493.pth\n",
            "\n",
            "Episode: 1494, Reward: -0.4577707432203575, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-29/q_network_ep_1494.pth\n",
            "\n",
            "Episode: 1495, Reward: -1.687105186305035, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-30/q_network_ep_1495.pth\n",
            "\n",
            "Episode: 1496, Reward: -2.081541288546029, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-31/q_network_ep_1496.pth\n",
            "\n",
            "Episode: 1497, Reward: -3.9788684234963196, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-32/q_network_ep_1497.pth\n",
            "\n",
            "Episode: 1498, Reward: -6.424946403562677, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-33/q_network_ep_1498.pth\n",
            "\n",
            "Episode: 1499, Reward: -8.18266933249237, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-34/q_network_ep_1499.pth\n",
            "\n",
            "Episode: 1500, Reward: -6.0680288937564555, Steps: 200\n",
            "model saved to models/2023-05-02_15-43-34/q_network_ep_1500.pth\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "\n",
        "# DO NOT CHANGE\n",
        "# ---------------\n",
        "arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01,\n",
        "\t    \t\t\tgravity=False\n",
        "        )\n",
        "    )\n",
        "arm.reset()\n",
        "env = ArmEnv(arm, gui=False)\n",
        "tqdn = TrainDQN(env)\n",
        "# ---------------\n",
        "\n",
        "# Call your trin function here\n",
        "tqdn.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To keep track of your experiments, it is good practice to plot and check how well is your model trained based on the returns vs episodes plot. With a large number of episodes, this  plot may look very jagged making it difficult to ascertain how well you are doing. We are proving code to smoothen out the plot by. This will take a large list of returns in every episode and plot a smoothened version of the list. Feel free to use it if it helps.\n",
        "```\n",
        "import seaborn as sns\n",
        "returns = __\n",
        "smoothing = 10\n",
        "\n",
        "smoothened = [sum(returns[i:i+smoothing])/smoothing for i in range(0, len(returns), smoothing)]\n",
        "sns.lineplot(smoothened)\n",
        "```"
      ],
      "metadata": {
        "id": "jAlaOcVJsn5a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIcBDbeTRNZI"
      },
      "source": [
        "### Load your model and test its performance\n",
        "Change your model path and the goal to see how well your learnt model is performing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5gTRNKhRQQM"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from render import Renderer\n",
        "from arm_env import ArmEnv\n",
        "import numpy as np\n",
        "import os\n",
        "from math import dist\n",
        "import seaborn as sns\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "from geometry import polar2cartesian\n",
        "\n",
        "\n",
        "# DO NOT CHANGE arm parameters\n",
        "arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01,\n",
        "\t    \t\t\tgravity=False\n",
        "        )\n",
        "    )\n",
        "arm.reset()\n",
        "# ------------------\n",
        "\n",
        "env = ArmEnv(arm, gui=False)\n",
        "model_path = '/content/models/2023-05-02_02-38-33/q_network_ep_1471.pth' # Fill in the model_path\n",
        "device = torch.device('cpu')\n",
        "qnet = QNetwork(env).to(device)\n",
        "qnet.load_state_dict(torch.load(model_path))\n",
        "qnet.eval()\n",
        "goal = polar2cartesian(1.6, 0.25 - np.pi/2.0)\n",
        "done = False\n",
        "obs = env.reset(goal)\n",
        "\n",
        "episode_return = 0\n",
        "while not done:\n",
        "  action = qnet.select_discrete_action(obs, device)\n",
        "  action = qnet.action_discrete_to_continuous(action)\n",
        "  new_obs, reward, done, info = env.step(action)\n",
        "  episode_return += reward\n",
        "\n",
        "  pos_ee = info['pos_ee']\n",
        "  vel_ee = info['vel_ee']\n",
        "  dist = np.linalg.norm(pos_ee - goal)\n",
        "\n",
        "  obs = new_obs\n",
        "print('Episode return: ', episode_return)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grading and Evaluation\n",
        "You will be evaluated on 5 different goal positions worth 1.5 points each. You must pass the best `model_path` for your network. The scoring function will run one episode for every goal position and find the total reward (aka return) for the episode. For every goal you get:\n",
        "\n",
        "* 1 Point if `easy target < total reward < hard target`\n",
        "* 1.5 Points if `hard target < total reward`"
      ],
      "metadata": {
        "id": "pUDEYLsZLSp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from score import compute_score\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from render import Renderer\n",
        "from arm_env import ArmEnv\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# DO NOT CHANGE arm parameters\n",
        "arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01,\n",
        "\t    \t\t\tgravity=False\n",
        "        )\n",
        "    )\n",
        "arm.reset()\n",
        "# ------------------\n",
        "\n",
        "env = ArmEnv(arm, gui=False)\n",
        "model_path = '/content/q_network_ep_1489.pth' # Fill in the model_path\n",
        "# model_path = '/content/models/2023-05-02_20-52-26/q_network_ep_1489.pth' # Fill in the model_path\n",
        "device = torch.device('cpu')\n",
        "qnet = QNetwork(env).to(device)\n",
        "qnet.load_state_dict(torch.load(model_path))\n",
        "qnet.eval()\n",
        "score = compute_score(qnet, env, device)"
      ],
      "metadata": {
        "id": "OhPD-u6TIxdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6bb7e47-e719-42a7-d1c6-c4c43f4ed954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Computing score---\n",
            "\n",
            "Goal 1:\n",
            "Total reward: -2.2054706825363968\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 1.5\n",
            "\n",
            "Goal 2:\n",
            "Total reward: -2.67815154496064\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 1.5\n",
            "\n",
            "Goal 3:\n",
            "Total reward: -2.523531654927499\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 1.5\n",
            "\n",
            "Goal 4:\n",
            "Total reward: -3.8442660235249067\n",
            "easy target: -7\n",
            "hard target: -5\n",
            "points: 1.5\n",
            "\n",
            "Goal 5:\n",
            "Total reward: -6.872121378657712\n",
            "easy target: -10\n",
            "hard target: -7\n",
            "points: 1.5\n",
            "\n",
            "\n",
            "Final score: 7.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: PPO with an open source RL library\n",
        "\n",
        "In this part, you will use one of the most popular open source RL libraries ([Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)) to solve the same goal reaching problem as Part 1. We will use the same `ArmEnv` gym environment. The algorithm you should choose to use is PPO."
      ],
      "metadata": {
        "id": "vkCvO0-05XK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PPO training\n",
        "\n",
        "We provide the code to construct parallel environments. Parallel environments can be very useful if you have good CPUs and it can speed up training."
      ],
      "metadata": {
        "id": "UBK89P2B8CgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
        "from stable_baselines3.common.vec_env.vec_monitor import VecMonitor\n",
        "from copy import deepcopy\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "from arm_env import ArmEnv\n",
        "\n",
        "class EnvMaker:\n",
        "    def __init__(self,  arm, seed):\n",
        "        self.seed = seed\n",
        "        self.arm = arm\n",
        "\n",
        "    def __call__(self):\n",
        "        arm = deepcopy(self.arm)\n",
        "        env = ArmEnv(arm)\n",
        "        env.seed(self.seed)\n",
        "        return env\n",
        "\n",
        "def make_vec_env(arm, nenv, seed):\n",
        "    return VecMonitor(SubprocVecEnv([EnvMaker(arm, seed  + 100 * i) for i in range(nenv)]))\n",
        "\n",
        "# conveniet function to create a robot arm\n",
        "def make_arm():\n",
        "    arm = Robot(\n",
        "        ArmDynamics(\n",
        "            num_links=2,\n",
        "            link_mass=0.1,\n",
        "            link_length=1,\n",
        "            joint_viscous_friction=0.1,\n",
        "            dt=0.01\n",
        "        )\n",
        "    )\n",
        "    arm.reset()\n",
        "    return arm\n"
      ],
      "metadata": {
        "id": "2RTqfmpVwMja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will need to complete the code to train the policy using the [PPO class](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) from stable_baselines3. We provide the code to generate the name of the directory to save the checkpoint, an example is `ppo_models/2023-04-13_01-14-13`. Your checkpoint model should be named `ppo_network.zip`. See the [save](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#stable_baselines3.ppo.PPO.save) function. Training should take less than 40 minutes."
      ],
      "metadata": {
        "id": "Bniz2TouwM3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.ppo import PPO\n",
        "import os\n",
        "import time\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "\n",
        "\n",
        "# Default parameters\n",
        "timesteps = 500000\n",
        "nenv = 8  # number of parallel environments. This can speed up training when you have good CPUs\n",
        "seed = 8\n",
        "batch_size = 2048\n",
        "\n",
        "# Generate path of the directory to save the checkpoint\n",
        "timestr = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "save_dir = os.path.join('ppo_models', timestr)\n",
        "\n",
        "# Set random seed\n",
        "set_random_seed(seed)\n",
        "\n",
        "# Create arm\n",
        "arm = make_arm()\n",
        "\n",
        "# Create parallel envs\n",
        "vec_env = make_vec_env(arm=arm, nenv=nenv, seed=seed)\n",
        "\n",
        "# ------ IMPLEMENT YOUR TRAINING CODE HERE ------------\n",
        "# Create the PPO model\n",
        "model = PPO(\"MlpPolicy\", vec_env, verbose=1, n_steps=batch_size, seed=seed)\n",
        "\n",
        "# Train the PPO model\n",
        "model.learn(total_timesteps=timesteps)\n",
        "\n",
        "# Save the trained model\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "model.save(os.path.join(save_dir, \"ppo_network.zip\"))\n",
        "\n",
        "# Do not forget to save your model at the end of training"
      ],
      "metadata": {
        "id": "FHoSWOnG-2sH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a7e26a-6cd7-4b06-9e51-3866e47540d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "-----------------------------------\n",
            "| rollout/           |            |\n",
            "|    ep_len_mean     | 200        |\n",
            "|    ep_rew_mean     | -200.53925 |\n",
            "| time/              |            |\n",
            "|    fps             | 820        |\n",
            "|    iterations      | 1          |\n",
            "|    time_elapsed    | 19         |\n",
            "|    total_timesteps | 16384      |\n",
            "-----------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -140.84149   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 653          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 50           |\n",
            "|    total_timesteps      | 32768        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0060855187 |\n",
            "|    clip_fraction        | 0.0604       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.85        |\n",
            "|    explained_variance   | 0.00222      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 31.5         |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00542     |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 242          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -89.754906   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 607          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 80           |\n",
            "|    total_timesteps      | 49152        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0069103083 |\n",
            "|    clip_fraction        | 0.08         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.85        |\n",
            "|    explained_variance   | 0.444        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 50.6         |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00801     |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 94.2         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -45.77525   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 580         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 112         |\n",
            "|    total_timesteps      | 65536       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008549693 |\n",
            "|    clip_fraction        | 0.105       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.83       |\n",
            "|    explained_variance   | 0.516       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 16.7        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0108     |\n",
            "|    std                  | 0.995       |\n",
            "|    value_loss           | 42.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -31.847864  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 569         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 143         |\n",
            "|    total_timesteps      | 81920       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008801997 |\n",
            "|    clip_fraction        | 0.113       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.8        |\n",
            "|    explained_variance   | 0.481       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.18        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0106     |\n",
            "|    std                  | 0.979       |\n",
            "|    value_loss           | 15.4        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -24.8396     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 564          |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 174          |\n",
            "|    total_timesteps      | 98304        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0076784734 |\n",
            "|    clip_fraction        | 0.103        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.77        |\n",
            "|    explained_variance   | 0.398        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.5          |\n",
            "|    n_updates            | 50           |\n",
            "|    policy_gradient_loss | -0.00896     |\n",
            "|    std                  | 0.966        |\n",
            "|    value_loss           | 7.36         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -22.477007  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 560         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 204         |\n",
            "|    total_timesteps      | 114688      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006741765 |\n",
            "|    clip_fraction        | 0.0775      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.74       |\n",
            "|    explained_variance   | 0.348       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.34        |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.00384    |\n",
            "|    std                  | 0.947       |\n",
            "|    value_loss           | 4.9         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -19.28473    |\n",
            "| time/                   |              |\n",
            "|    fps                  | 555          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 235          |\n",
            "|    total_timesteps      | 131072       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0064207986 |\n",
            "|    clip_fraction        | 0.074        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.71        |\n",
            "|    explained_variance   | 0.452        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.58         |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.00248     |\n",
            "|    std                  | 0.939        |\n",
            "|    value_loss           | 3.68         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -16.117456   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 546          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 269          |\n",
            "|    total_timesteps      | 147456       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0064843106 |\n",
            "|    clip_fraction        | 0.0732       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.7         |\n",
            "|    explained_variance   | 0.589        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.57         |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -0.00347     |\n",
            "|    std                  | 0.933        |\n",
            "|    value_loss           | 2.98         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -14.899771   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 544          |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 300          |\n",
            "|    total_timesteps      | 163840       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0065595917 |\n",
            "|    clip_fraction        | 0.073        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.69        |\n",
            "|    explained_variance   | 0.636        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.66         |\n",
            "|    n_updates            | 90           |\n",
            "|    policy_gradient_loss | -0.00178     |\n",
            "|    std                  | 0.932        |\n",
            "|    value_loss           | 2.5          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -14.010608   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 543          |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 331          |\n",
            "|    total_timesteps      | 180224       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0073391586 |\n",
            "|    clip_fraction        | 0.072        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.67        |\n",
            "|    explained_variance   | 0.748        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.32         |\n",
            "|    n_updates            | 100          |\n",
            "|    policy_gradient_loss | -0.00197     |\n",
            "|    std                  | 0.916        |\n",
            "|    value_loss           | 1.71         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -14.185586  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 543         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 361         |\n",
            "|    total_timesteps      | 196608      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007520535 |\n",
            "|    clip_fraction        | 0.0909      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.64       |\n",
            "|    explained_variance   | 0.796       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.72        |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.00394    |\n",
            "|    std                  | 0.901       |\n",
            "|    value_loss           | 1.64        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 200        |\n",
            "|    ep_rew_mean          | -12.894517 |\n",
            "| time/                   |            |\n",
            "|    fps                  | 543        |\n",
            "|    iterations           | 13         |\n",
            "|    time_elapsed         | 391        |\n",
            "|    total_timesteps      | 212992     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00839888 |\n",
            "|    clip_fraction        | 0.102      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.61      |\n",
            "|    explained_variance   | 0.78       |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.636      |\n",
            "|    n_updates            | 120        |\n",
            "|    policy_gradient_loss | -0.00588   |\n",
            "|    std                  | 0.889      |\n",
            "|    value_loss           | 1.36       |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -10.453338   |\n",
            "| time/                   |              |\n",
            "|    fps                  | 543          |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 422          |\n",
            "|    total_timesteps      | 229376       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0084632775 |\n",
            "|    clip_fraction        | 0.0974       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.58        |\n",
            "|    explained_variance   | 0.768        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.638        |\n",
            "|    n_updates            | 130          |\n",
            "|    policy_gradient_loss | -0.00648     |\n",
            "|    std                  | 0.875        |\n",
            "|    value_loss           | 1.59         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -8.037116   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 541         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 453         |\n",
            "|    total_timesteps      | 245760      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009748412 |\n",
            "|    clip_fraction        | 0.12        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.52       |\n",
            "|    explained_variance   | 0.822       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.187       |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.00596    |\n",
            "|    std                  | 0.855       |\n",
            "|    value_loss           | 0.586       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -6.6329827  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 540         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 485         |\n",
            "|    total_timesteps      | 262144      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009424352 |\n",
            "|    clip_fraction        | 0.114       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.5        |\n",
            "|    explained_variance   | 0.847       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.118       |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.00628    |\n",
            "|    std                  | 0.842       |\n",
            "|    value_loss           | 0.313       |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 200        |\n",
            "|    ep_rew_mean          | -5.9764595 |\n",
            "| time/                   |            |\n",
            "|    fps                  | 539        |\n",
            "|    iterations           | 17         |\n",
            "|    time_elapsed         | 516        |\n",
            "|    total_timesteps      | 278528     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00905014 |\n",
            "|    clip_fraction        | 0.103      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -2.47      |\n",
            "|    explained_variance   | 0.846      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.0725     |\n",
            "|    n_updates            | 160        |\n",
            "|    policy_gradient_loss | -0.00492   |\n",
            "|    std                  | 0.835      |\n",
            "|    value_loss           | 0.244      |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -5.3366485  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 539         |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 547         |\n",
            "|    total_timesteps      | 294912      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010540191 |\n",
            "|    clip_fraction        | 0.126       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.43       |\n",
            "|    explained_variance   | 0.867       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0979      |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.0071     |\n",
            "|    std                  | 0.815       |\n",
            "|    value_loss           | 0.129       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -4.3607783  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 538         |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 577         |\n",
            "|    total_timesteps      | 311296      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011563506 |\n",
            "|    clip_fraction        | 0.147       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.38       |\n",
            "|    explained_variance   | 0.896       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0296      |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | -0.00883    |\n",
            "|    std                  | 0.797       |\n",
            "|    value_loss           | 0.0819      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -3.8999066  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 536         |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 610         |\n",
            "|    total_timesteps      | 327680      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010548554 |\n",
            "|    clip_fraction        | 0.12        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.33       |\n",
            "|    explained_variance   | 0.883       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0359      |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | -0.003      |\n",
            "|    std                  | 0.776       |\n",
            "|    value_loss           | 0.0451      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -3.8784068  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 536         |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 641         |\n",
            "|    total_timesteps      | 344064      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010465087 |\n",
            "|    clip_fraction        | 0.118       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.27       |\n",
            "|    explained_variance   | 0.907       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.00422     |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.00358    |\n",
            "|    std                  | 0.753       |\n",
            "|    value_loss           | 0.0353      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -3.4933536  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 536         |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 672         |\n",
            "|    total_timesteps      | 360448      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009691468 |\n",
            "|    clip_fraction        | 0.115       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.21       |\n",
            "|    explained_variance   | 0.912       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0019      |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.00308    |\n",
            "|    std                  | 0.73        |\n",
            "|    value_loss           | 0.0256      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -3.038329   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 536         |\n",
            "|    iterations           | 23          |\n",
            "|    time_elapsed         | 702         |\n",
            "|    total_timesteps      | 376832      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012613339 |\n",
            "|    clip_fraction        | 0.133       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.14       |\n",
            "|    explained_variance   | 0.929       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.012       |\n",
            "|    n_updates            | 220         |\n",
            "|    policy_gradient_loss | -0.00267    |\n",
            "|    std                  | 0.705       |\n",
            "|    value_loss           | 0.0177      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -3.133666   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 536         |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 733         |\n",
            "|    total_timesteps      | 393216      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011581661 |\n",
            "|    clip_fraction        | 0.134       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.07       |\n",
            "|    explained_variance   | 0.959       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0235     |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.00253    |\n",
            "|    std                  | 0.679       |\n",
            "|    value_loss           | 0.00988     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -3.2157986  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 534         |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 766         |\n",
            "|    total_timesteps      | 409600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012866721 |\n",
            "|    clip_fraction        | 0.15        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2          |\n",
            "|    explained_variance   | 0.964       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.00602     |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.00346    |\n",
            "|    std                  | 0.654       |\n",
            "|    value_loss           | 0.00844     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -2.8539004  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 533         |\n",
            "|    iterations           | 26          |\n",
            "|    time_elapsed         | 798         |\n",
            "|    total_timesteps      | 425984      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013298731 |\n",
            "|    clip_fraction        | 0.154       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.93       |\n",
            "|    explained_variance   | 0.97        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0138      |\n",
            "|    n_updates            | 250         |\n",
            "|    policy_gradient_loss | -0.00161    |\n",
            "|    std                  | 0.634       |\n",
            "|    value_loss           | 0.00611     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -3.0847747  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 533         |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 828         |\n",
            "|    total_timesteps      | 442368      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013670282 |\n",
            "|    clip_fraction        | 0.162       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.87       |\n",
            "|    explained_variance   | 0.979       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0215     |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | -0.00143    |\n",
            "|    std                  | 0.617       |\n",
            "|    value_loss           | 0.00542     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -2.894516   |\n",
            "| time/                   |             |\n",
            "|    fps                  | 534         |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 859         |\n",
            "|    total_timesteps      | 458752      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010958417 |\n",
            "|    clip_fraction        | 0.144       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.82       |\n",
            "|    explained_variance   | 0.983       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.00801    |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | 0.000513    |\n",
            "|    std                  | 0.6         |\n",
            "|    value_loss           | 0.00338     |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 200        |\n",
            "|    ep_rew_mean          | -2.6819742 |\n",
            "| time/                   |            |\n",
            "|    fps                  | 533        |\n",
            "|    iterations           | 29         |\n",
            "|    time_elapsed         | 890        |\n",
            "|    total_timesteps      | 475136     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.0134814  |\n",
            "|    clip_fraction        | 0.153      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.76      |\n",
            "|    explained_variance   | 0.989      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | -0.0172    |\n",
            "|    n_updates            | 280        |\n",
            "|    policy_gradient_loss | 0.00189    |\n",
            "|    std                  | 0.585      |\n",
            "|    value_loss           | 0.00254    |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -2.4431663  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 532         |\n",
            "|    iterations           | 30          |\n",
            "|    time_elapsed         | 922         |\n",
            "|    total_timesteps      | 491520      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011919153 |\n",
            "|    clip_fraction        | 0.168       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.72       |\n",
            "|    explained_variance   | 0.989       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0192     |\n",
            "|    n_updates            | 290         |\n",
            "|    policy_gradient_loss | -0.000783   |\n",
            "|    std                  | 0.575       |\n",
            "|    value_loss           | 0.00241     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -2.5098772  |\n",
            "| time/                   |             |\n",
            "|    fps                  | 532         |\n",
            "|    iterations           | 31          |\n",
            "|    time_elapsed         | 953         |\n",
            "|    total_timesteps      | 507904      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017641885 |\n",
            "|    clip_fraction        | 0.18        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.67       |\n",
            "|    explained_variance   | 0.988       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0187     |\n",
            "|    n_updates            | 300         |\n",
            "|    policy_gradient_loss | 0.000858    |\n",
            "|    std                  | 0.558       |\n",
            "|    value_loss           | 0.00194     |\n",
            "-----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grading and evaluation\n",
        "\n",
        "The total number of points for Part 2 is 7.5. We will evaluate your trained model on 5 random goal locations. For each test, we assign points based on the distance between the end effector and the goal location at the end of the episode.\n",
        "\n",
        "- If 0 < distance < 0.05, you get 1.5 points.\n",
        "- If 0.05 <= distance < 0.1, you get 1 point.\n",
        "- If distance >= 0.1, you get 0 point.\n",
        "\n"
      ],
      "metadata": {
        "id": "f9N2falIz9rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from score import score_policy\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "from robot import Robot\n",
        "from arm_dynamics import ArmDynamics\n",
        "from render import Renderer\n",
        "import time\n",
        "\n",
        "# Set the path to your model\n",
        "# model_path = 'ppo_network.zip'\n",
        "model_path = '/content/ppo_models/2023-05-02_20-12-11/ppo_network.zip'\n",
        "\n",
        "set_random_seed(seed=100)\n",
        "\n",
        "# Create arm robot\n",
        "arm = make_arm()\n",
        "\n",
        "# Create environment\n",
        "env = ArmEnv(arm, gui=False)\n",
        "env.seed(100)\n",
        "\n",
        "# Load and test policy\n",
        "policy = PPO.load(model_path)\n",
        "score_policy(policy, env)"
      ],
      "metadata": {
        "id": "X6eQ2mzglwd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76da8d37-27a8-4a23-b86c-0ee647360c53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Computing score ---\n",
            "\n",
            "Goal 1: 1.5\n",
            "\n",
            "Goal 2: 1.5\n",
            "\n",
            "Goal 3: 1.5\n",
            "\n",
            "Goal 4: 1.5\n",
            "\n",
            "Goal 5: 1.5\n",
            "\n",
            "\n",
            "---\n",
            "Final score: 7.5/7.5\n",
            "---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.5"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}